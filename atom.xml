<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AI Ants</title>
  
  <subtitle>迈向通用人工智能之路</subtitle>
  <link href="https://yangsuhui.github.io/atom.xml" rel="self"/>
  
  <link href="https://yangsuhui.github.io/"/>
  <updated>2021-01-09T09:57:22.332Z</updated>
  <id>https://yangsuhui.github.io/</id>
  
  <author>
    <name>yangsuhui</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>TIES工程训练及表格的生成</title>
    <link href="https://yangsuhui.github.io/p/1c63.html"/>
    <id>https://yangsuhui.github.io/p/1c63.html</id>
    <published>2021-01-09T09:16:43.000Z</published>
    <updated>2021-01-09T09:57:22.332Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>python多线程/多进程以及压测</title>
    <link href="https://yangsuhui.github.io/p/3ae3.html"/>
    <id>https://yangsuhui.github.io/p/3ae3.html</id>
    <published>2021-01-08T03:55:01.000Z</published>
    <updated>2021-01-09T09:57:22.333Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>医学影像算法总结</title>
    <link href="https://yangsuhui.github.io/p/adb6.html"/>
    <id>https://yangsuhui.github.io/p/adb6.html</id>
    <published>2021-01-08T03:12:34.000Z</published>
    <updated>2021-01-09T09:57:22.335Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>神经网络常用计算模块总结</title>
    <link href="https://yangsuhui.github.io/p/6fb7.html"/>
    <id>https://yangsuhui.github.io/p/6fb7.html</id>
    <published>2021-01-08T03:07:22.000Z</published>
    <updated>2021-01-09T09:57:22.346Z</updated>
    
    <content type="html"><![CDATA[<p>1、Flops、MACs、Params计算</p><p><a href="https://github.com/he-y/soft-filter-pruning/blob/master/utils/cifar_resnet_flop.py">https://github.com/he-y/soft-filter-pruning/blob/master/utils/cifar_resnet_flop.py</a></p><p><a href="https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py">https://github.com/warmspringwinds/pytorch-segmentation-detection/blob/master/pytorch_segmentation_detection/utils/flops_benchmark.py</a></p><p><a href="https://github.com/Lyken17/pytorch-OpCounter">https://github.com/Lyken17/pytorch-OpCounter</a></p><p>crnn中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">统计模型参数量等信息</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_info</span>(<span class="params">model</span>):</span>  <span class="comment"># Plots a line-by-line description of a PyTorch model</span></span><br><span class="line">    n_p = sum(x.numel() <span class="keyword">for</span> x <span class="keyword">in</span> model.parameters())  <span class="comment"># number parameters</span></span><br><span class="line">    n_g = sum(x.numel() <span class="keyword">for</span> x <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> x.requires_grad)  <span class="comment"># number gradients</span></span><br><span class="line">    print(<span class="string">&#x27;\n%5s %50s %9s %12s %20s %12s %12s&#x27;</span> % (<span class="string">&#x27;layer&#x27;</span>, <span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;gradient&#x27;</span>, <span class="string">&#x27;parameters&#x27;</span>, <span class="string">&#x27;shape&#x27;</span>, <span class="string">&#x27;mu&#x27;</span>, <span class="string">&#x27;sigma&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> i, (name, p) <span class="keyword">in</span> enumerate(model.named_parameters()):</span><br><span class="line">        name = name.replace(<span class="string">&#x27;module_list.&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">        print(<span class="string">&#x27;%5g %50s %9s %12g %20s %12.3g %12.3g&#x27;</span> % (</span><br><span class="line">            i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))</span><br><span class="line">    print(<span class="string">&#x27;Model Summary: %g layers, %g parameters, %g gradients\n&#x27;</span> % (i + <span class="number">1</span>, n_p, n_g))</span><br></pre></td></tr></table></figure><p>2、模型初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weights_init</span>(<span class="params">model</span>):</span></span><br><span class="line">    <span class="comment"># Official init from torch repo.</span></span><br><span class="line">    <span class="keyword">for</span> m <span class="keyword">in</span> model.modules():</span><br><span class="line">        <span class="keyword">if</span> isinstance(m, nn.Conv2d):</span><br><span class="line">            nn.init.kaiming_normal_(m.weight)</span><br><span class="line">        <span class="keyword">elif</span> isinstance(m, nn.BatchNorm2d):</span><br><span class="line">            nn.init.constant_(m.weight, <span class="number">1</span>)</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">elif</span> isinstance(m, nn.Linear):</span><br><span class="line">            nn.init.constant_(m.bias, <span class="number">0</span>)</span><br><span class="line">encoder = crnn.CNN(opt.imgH, nc, opt.nh, cfg, mode=<span class="string">&#x27;2D&#x27;</span>,dim_in=<span class="number">512</span>)</span><br><span class="line">encoder.apply(weights_init)</span><br></pre></td></tr></table></figure><p>3、导入模型(单机单卡到分布式)</p><p>4、自定义学习率(如warmup)和优化器</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、Flops、MACs、Params计算&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/he-y/soft-filter-pruning/blob/master/utils/cifar_resnet_flop.py&quot;&gt;https://github.</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>度量学习及图像检索工程应用</title>
    <link href="https://yangsuhui.github.io/p/3cef.html"/>
    <id>https://yangsuhui.github.io/p/3cef.html</id>
    <published>2021-01-07T08:59:58.000Z</published>
    <updated>2021-01-07T11:20:40.557Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>GAN变种及应用汇总</title>
    <link href="https://yangsuhui.github.io/p/eac6.html"/>
    <id>https://yangsuhui.github.io/p/eac6.html</id>
    <published>2021-01-07T08:58:22.000Z</published>
    <updated>2021-01-08T03:43:32.198Z</updated>
    
    <content type="html"><![CDATA[<h2 id="网络变种"><a href="#网络变种" class="headerlink" title="网络变种"></a>网络变种</h2><p>1、PyTorch-GAN</p><p><a href="https://github.com/eriklindernoren/PyTorch-GAN">https://github.com/eriklindernoren/PyTorch-GAN</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">Table of Contents</span><br><span class="line">Installation</span><br><span class="line">Implementations</span><br><span class="line">Auxiliary Classifier GAN</span><br><span class="line">Adversarial Autoencoder</span><br><span class="line">BEGAN</span><br><span class="line">BicycleGAN</span><br><span class="line">Boundary-Seeking GAN</span><br><span class="line">Cluster GAN</span><br><span class="line">Conditional GAN</span><br><span class="line">Context-Conditional GAN</span><br><span class="line">Context Encoder</span><br><span class="line">Coupled GAN</span><br><span class="line">CycleGAN</span><br><span class="line">Deep Convolutional GAN</span><br><span class="line">DiscoGAN</span><br><span class="line">DRAGAN</span><br><span class="line">DualGAN</span><br><span class="line">Energy-Based GAN</span><br><span class="line">Enhanced Super-Resolution GAN</span><br><span class="line">GAN</span><br><span class="line">InfoGAN</span><br><span class="line">Least Squares GAN</span><br><span class="line">MUNIT</span><br><span class="line">Pix2Pix</span><br><span class="line">PixelDA</span><br><span class="line">Relativistic GAN</span><br><span class="line">Semi-Supervised GAN</span><br><span class="line">Softmax GAN</span><br><span class="line">StarGAN</span><br><span class="line">Super-Resolution GAN</span><br><span class="line">UNIT</span><br><span class="line">Wasserstein GAN</span><br><span class="line">Wasserstein GAN GP</span><br><span class="line">Wasserstein GAN DIV</span><br></pre></td></tr></table></figure><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><h3 id="风格迁移"><a href="#风格迁移" class="headerlink" title="风格迁移"></a>风格迁移</h3><p>1、pystiche</p><p><a href="https://github.com/pmeier/pystiche">https://github.com/pmeier/pystiche</a></p><p>Framework for Neural Style Transfer (NST) built upon PyTorch</p><p>2、GMA_CI_2019_ssim_content_loss</p><p><a href="https://github.com/pmeier/GMA_CI_2019_ssim_content_loss">https://github.com/pmeier/GMA_CI_2019_ssim_content_loss</a></p><p>Content representation for Neural Style Transfer Algorithms based on Structural Similarity</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;网络变种&quot;&gt;&lt;a href=&quot;#网络变种&quot; class=&quot;headerlink&quot; title=&quot;网络变种&quot;&gt;&lt;/a&gt;网络变种&lt;/h2&gt;&lt;p&gt;1、PyTorch-GAN&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/eriklindernor</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>数据/类别不均衡分析</title>
    <link href="https://yangsuhui.github.io/p/5a32.html"/>
    <id>https://yangsuhui.github.io/p/5a32.html</id>
    <published>2021-01-07T08:57:23.000Z</published>
    <updated>2021-01-09T02:55:26.281Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>目标检测模型优化</title>
    <link href="https://yangsuhui.github.io/p/65b8.html"/>
    <id>https://yangsuhui.github.io/p/65b8.html</id>
    <published>2021-01-07T08:45:02.000Z</published>
    <updated>2021-01-07T11:20:40.569Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Anchor的生成和匹配-正负样本的选取"><a href="#Anchor的生成和匹配-正负样本的选取" class="headerlink" title="Anchor的生成和匹配(正负样本的选取)"></a>Anchor的生成和匹配(正负样本的选取)</h2><h2 id="NMS"><a href="#NMS" class="headerlink" title="NMS"></a>NMS</h2><h2 id="Loss"><a href="#Loss" class="headerlink" title="Loss"></a>Loss</h2><h2 id="样本-类别-不均衡-长尾分布"><a href="#样本-类别-不均衡-长尾分布" class="headerlink" title="样本(类别)不均衡(长尾分布)"></a>样本(类别)不均衡(长尾分布)</h2><h2 id="特征对齐"><a href="#特征对齐" class="headerlink" title="特征对齐"></a>特征对齐</h2><p>1、ROI pooling</p><h2 id="小目标检测优化"><a href="#小目标检测优化" class="headerlink" title="小目标检测优化"></a>小目标检测优化</h2><h2 id="细长目标检测优化"><a href="#细长目标检测优化" class="headerlink" title="细长目标检测优化"></a>细长目标检测优化</h2><h2 id="速度和精度的trade-off"><a href="#速度和精度的trade-off" class="headerlink" title="速度和精度的trade off"></a>速度和精度的trade off</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Anchor的生成和匹配-正负样本的选取&quot;&gt;&lt;a href=&quot;#Anchor的生成和匹配-正负样本的选取&quot; class=&quot;headerlink&quot; title=&quot;Anchor的生成和匹配(正负样本的选取)&quot;&gt;&lt;/a&gt;Anchor的生成和匹配(正负样本的选取)&lt;/h2</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>模型压缩(蒸馏/剪枝/量化)方法总结</title>
    <link href="https://yangsuhui.github.io/p/f647.html"/>
    <id>https://yangsuhui.github.io/p/f647.html</id>
    <published>2021-01-07T08:36:05.000Z</published>
    <updated>2021-01-08T03:05:53.874Z</updated>
    
    <content type="html"><![CDATA[<h2 id="蒸馏"><a href="#蒸馏" class="headerlink" title="蒸馏"></a>蒸馏</h2><p>1、Distilling Object Detectors with Fine-grained Feature Imitation(CVPR19)</p><p><a href="https://github.com/twangnh/Distilling-Object-Detectors">https://github.com/twangnh/Distilling-Object-Detectors</a></p><h2 id="剪枝"><a href="#剪枝" class="headerlink" title="剪枝"></a>剪枝</h2><p>1、soft-filter-pruning(FPGM)</p><p><a href="https://github.com/he-y/filter-pruning-geometric-median?utm_source=catalyzex.com">https://github.com/he-y/filter-pruning-geometric-median?utm_source=catalyzex.com</a></p><p>Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration(CVPR19 Oral)</p><h2 id="量化"><a href="#量化" class="headerlink" title="量化"></a>量化</h2><p>1、EasyQuant: Post-training Quantization via Scale Optimization</p><p><a href="https://github.com/deepglint/EasyQuant">https://github.com/deepglint/EasyQuant</a></p><p>EasyQuant(EQ) is an efficient and simple post-training quantization method via effectively optimizing the scales of weights and activations</p><p>2、dnn-gating(PACT)</p><p><a href="https://github.com/cornell-zhang/dnn-gating?utm_source=catalyzex.com">https://github.com/cornell-zhang/dnn-gating?utm_source=catalyzex.com</a></p><p>PACT: PARAMETERIZED CLIPPING ACTIVATION FOR QUANTIZED NEURAL NETWORKS</p><p>3、scale-adjusted-training</p><p><a href="https://github.com/jakc4103/scale-adjusted-training?utm_source=catalyzex.com">https://github.com/jakc4103/scale-adjusted-training?utm_source=catalyzex.com</a></p><p>Towards Efficient Training for Neural Network Quantization </p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;蒸馏&quot;&gt;&lt;a href=&quot;#蒸馏&quot; class=&quot;headerlink&quot; title=&quot;蒸馏&quot;&gt;&lt;/a&gt;蒸馏&lt;/h2&gt;&lt;p&gt;1、Distilling Object Detectors with Fine-grained Feature Imitation(CVPR</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>pytorch加速(数据读取/训练加速/推理加速)</title>
    <link href="https://yangsuhui.github.io/p/8e50.html"/>
    <id>https://yangsuhui.github.io/p/8e50.html</id>
    <published>2021-01-07T06:31:05.000Z</published>
    <updated>2021-01-07T08:16:27.347Z</updated>
    
    <content type="html"><![CDATA[<p>1、FairScale</p><p><a href="https://github.com/facebookresearch/fairscale">https://github.com/facebookresearch/fairscale</a></p><p>FairScale is a PyTorch extension library for high performance and large scale training on one or multiple machines/nodes. This library extends basic PyTorch capabilities while adding new experimental ones.</p><p>2、SpeedTorch</p><p><a href="https://github.com/Santosh-Gupta/SpeedTorch">https://github.com/Santosh-Gupta/SpeedTorch</a></p><p>This library revovles around Cupy tensors pinned to CPU, which can achieve <strong>3.1x</strong> faster CPU -&gt; GPU transfer than regular Pytorch Pinned CPU tensors can, and <strong>410x</strong> faster GPU -&gt; CPU transfer. Speed depends on amount of data, and number of CPU cores on your system (see the How it Works section for more details)</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://zhuanlan.zhihu.com/p/161246287">Facebook发布FAIRScale：用于高性能和大规模训练的PyTorch工具</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、FairScale&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/facebookresearch/fairscale&quot;&gt;https://github.com/facebookresearch/fairscale&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FairS</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>OCR/文本纠错方法汇总</title>
    <link href="https://yangsuhui.github.io/p/9305.html"/>
    <id>https://yangsuhui.github.io/p/9305.html</id>
    <published>2021-01-07T03:22:21.000Z</published>
    <updated>2021-01-07T07:39:22.517Z</updated>
    
    <content type="html"><![CDATA[<p>1、pycorrector</p><p><a href="https://github.com/shibing624/pycorrector">https://github.com/shibing624/pycorrector</a></p><p>中文文本纠错工具。音似、形似错字（或变体字）纠正，可用于中文拼音、笔画输入法的错误纠正。python3.6开发。</p><p><strong>pycorrector</strong>依据语言模型检测错别字位置，通过拼音音似特征、笔画五笔编辑距离特征及语言模型困惑度特征纠正错别字。</p><h2 id="Question"><a href="#Question" class="headerlink" title="Question"></a>Question</h2><p>中文文本纠错任务，常见错误类型包括：</p><ul><li>谐音字词，如 配副眼睛-配副眼镜</li><li>混淆音字词，如 流浪织女-牛郎织女</li><li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li><li>字词补全，如 爱有天意-假如爱有天意</li><li>形似字错误，如 高梁-高粱</li><li>中文拼音全拼，如 xingfu-幸福</li><li>中文拼音缩写，如 sz-深圳</li><li>语法错误，如 想象难以-难以想象</li></ul><p>当然，针对不同业务场景，这些问题并不一定全部存在，比如输入法中需要处理前四种，搜索引擎需要处理所有类型，语音识别后文本纠错只需要处理前两种， 其中’形似字错误’主要针对五笔或者笔画手写输入等。本项目重点解决其中的谐音、混淆音、形似字错误、中文拼音全拼、语法错误带来的纠错任务。</p><p>2、FASPell</p><p><a href="https://github.com/iqiyi/FASPell">https://github.com/iqiyi/FASPell</a></p><p>This repository (licensed under GNU General Public License v3.0) contains all the data and code you need to build a state-of-the-art (by early 2019) Chinese spell checker and replicate the experiments in the original paper:</p><p><strong>FASPell: A Fast, Adaptable, Simple, Powerful Chinese Spell Checker Based On DAE-Decoder Paradigm</strong> <a href="https://www.aclweb.org/anthology/D19-5522.pdf">LINK</a></p><p>, which is published in the Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text.</p><p>2-1、APTED algorithm for the Tree Edit Distance</p><p><a href="https://github.com/DatabaseGroup/apted">https://github.com/DatabaseGroup/apted</a></p><p>Note that FASPell only adopts <strong>string edit distance to compute similarity.</strong> If you are interested in using <strong>tree edit distance to compute similarity</strong>, you need to download (from <a href="https://github.com/DatabaseGroup/apted">here</a>) and compile a tree edit distance executable <code>apted.jar</code> into the home directory before running:</p><p>3、OCR-Corrector</p><p><a href="https://github.com/tiantian91091317/OCR-Corrector">https://github.com/tiantian91091317/OCR-Corrector</a></p><p>利用语言模型，纠正OCR识别错误</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://zhuanlan.zhihu.com/p/179957371">针对OCR的NLP纠错：从原理到实践</a></p><p><a href="https://zhuanlan.zhihu.com/p/159101860">AI LIVE | 文本纠错技术探索和实践</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;OCR识别模型的输出一般存在一定数目的识别错误，且很难再在识别模型端提升效果，因此利用NLP技术进行文本纠错至关重要。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yangsuhui.github.io/categories/Deep-Learning/"/>
    
    
    <category term="文本纠错" scheme="https://yangsuhui.github.io/tags/%E6%96%87%E6%9C%AC%E7%BA%A0%E9%94%99/"/>
    
    <category term="OCR" scheme="https://yangsuhui.github.io/tags/OCR/"/>
    
  </entry>
  
  <entry>
    <title>文档理解(版面分析/表格检测/表格结构解析)方法及数据</title>
    <link href="https://yangsuhui.github.io/p/3238.html"/>
    <id>https://yangsuhui.github.io/p/3238.html</id>
    <published>2021-01-07T02:43:37.000Z</published>
    <updated>2021-01-07T07:36:56.907Z</updated>
    
    <content type="html"><![CDATA[<p>1、PubLayNet</p><p><strong>PubLayNet is a large dataset of document images, of which the layout is annotated with both bounding boxes and polygonal segmentations.</strong></p><p><a href="https://github.com/ibm-aur-nlp/PubLayNet">https://github.com/ibm-aur-nlp/PubLayNet</a></p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210107105028.png" alt="annotations" style="zoom: 200%;" /><p>2、PubTabNet</p><p><strong>PubTabNet is a large dataset for image-based table recognition, containing 568k+ images of tabular data annotated with the corresponding HTML representation of the tables</strong></p><p><a href="https://github.com/ibm-aur-nlp/PubTabNet">https://github.com/ibm-aur-nlp/PubTabNet</a></p><p>Image-based table recognition: data, model, and evaluation：ECCV20</p><p>2-1、Tree-Edit-Distance-based Similarity (TEDS)</p><p>Evaluation metric for table recognition. This metric measures both the structure similarity and the cell content similarity between the prediction and the ground truth. The score is normalized between 0 and 1, where 1 means perfect matching.</p><p><a href="https://github.com/ibm-aur-nlp/PubTabNet/tree/master/src">https://github.com/ibm-aur-nlp/PubTabNet/tree/master/src</a></p><p>github group：<a href="https://github.com/ibm-aur-nlp">https://github.com/ibm-aur-nlp</a></p><p>2-2、dataset-tools</p><p>Java command-line tools for comparing results to ground truth for table location and structure detection as used in the ICDAR 2013 Table Competition.</p><p><a href="https://github.com/tamirhassan/dataset-tools">https://github.com/tamirhassan/dataset-tools</a></p><p>3、DocBank</p><p>DocBank is a new large-scale dataset that is constructed using a weak supervision approach. It enables models to integrate both the textual and layout information for downstream tasks. The current DocBank dataset totally includes 500K document pages, where 400K for training, 50K for validation and 50K for testing.</p><p><a href="https://github.com/doc-analysis/DocBank">https://github.com/doc-analysis/DocBank</a></p><p><strong>We provide a dataset loader named <a href="https://github.com/doc-analysis/DocBankLoader">DocBankLoader</a> and it can also convert DocBank to the Object Detection models’ format</strong> </p><p><strong><em>LayoutLM</em></strong> (<a href="https://github.com/microsoft/unilm/tree/master/layoutlm">repo</a>, <a href="https://arxiv.org/abs/1912.13318">paper</a>) is an effective pre-training method of text and layout and archives the SOTA result on DocBank</p><p>4、TableBank</p><p><a href="https://github.com/doc-analysis/TableBank">https://github.com/doc-analysis/TableBank</a></p><p><strong>We release an official split for the train/val/test datasets and re-train both of the Table Detection and Table Structure Recognition models using Detectron2 and OpenNMT tools. The benchmark results, the MODEL ZOO, and the download link of TableBank have been updated.</strong> </p><h2 id="Task-Definition"><a href="#Task-Definition" class="headerlink" title="Task Definition"></a>Task Definition</h2><h3 id="Table-Detection"><a href="#Table-Detection" class="headerlink" title="Table Detection"></a>Table Detection</h3><p>Table detection aims to locate tables using bounding boxes in a document. Given a document page in the image format, generating several bounding box that represents the location of tables in this page.</p><h3 id="Table-Structure-Recognition"><a href="#Table-Structure-Recognition" class="headerlink" title="Table Structure Recognition"></a>Table Structure Recognition</h3><p>Table structure recognition aims to identify the row and column layout structure for the tables especially in non-digital document formats such as scanned images. Given a table in the image format, generating an HTML tag sequence that represents the arrangement of rows and columns as well as the type of table cells.</p><p>github group：<a href="https://github.com/doc-analysis">https://github.com/doc-analysis</a></p><p>5、DocumentLayoutAnalysis</p><p><a href="https://github.com/BobLd/DocumentLayoutAnalysis">https://github.com/BobLd/DocumentLayoutAnalysis</a></p><p>文档解析(包括表格)资料汇总</p><p>5-1、SciTSR</p><p><a href="https://github.com/Academic-Hammer/SciTSR">https://github.com/Academic-Hammer/SciTSR</a></p><p>Table structure recognition dataset of the paper: Complicated Table Structure Recognition</p><p>SciTSR is a large-scale table structure recognition dataset, which contains 15,000 tables in PDF format and their corresponding structure labels obtained from LaTeX source files.</p><p>5-2、DocParser</p><p><a href="https://github.com/DS3Lab/DocParser/">https://github.com/DS3Lab/DocParser/</a></p><p>Hierarchical Structure Parsing of Document Renderings</p><p>6、TIES-2.0</p><p><a href="https://github.com/shahrukhqasim/TIES-2.0">https://github.com/shahrukhqasim/TIES-2.0</a></p><p>Table Information Extraction System</p><p>Rethinking Table Recognition using Graph Neural Networks：ICDAR19</p><p>6-1、TIES_DataGeneration(TIES-2.0对应的数据生成)</p><p><a href="https://github.com/hassan-mahmood/TIES_DataGeneration">https://github.com/hassan-mahmood/TIES_DataGeneration</a></p><p>7、CascadeTabNet</p><p><a href="https://github.com/DevashishPrasad/CascadeTabNet">https://github.com/DevashishPrasad/CascadeTabNet</a></p><p><strong>CascadeTabNet: An approach for end to end table detection and structure recognition from image-based documents</strong></p><p>The paper was presented (Orals) at <a href="https://cvpr2020text.wordpress.com/">CVPR 2020 Workshop on Text and Documents in the Deep Learning Era</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210107143905.gif" alt="CVPR Teaser"></p><p>8、GFTE</p><p><a href="https://github.com/Irene323/GFTE">https://github.com/Irene323/GFTE</a></p><p>GFTE: Graph-based Financial Table Extraction</p><p>9、GANs for tabular data</p><p>We well know GANs for success in the realistic image generation. However, they can be applied in tabular data generation. </p><p><a href="https://github.com/Diyago/GAN-for-tabular-data">https://github.com/Diyago/GAN-for-tabular-data</a></p><p><a href="https://towardsdatascience.com/review-of-gans-for-tabular-data-a30a2199342">https://towardsdatascience.com/review-of-gans-for-tabular-data-a30a2199342</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;现有的版面分析聚焦在版面的各个item的检测定位，但是进一步的文档理解进展缓慢，特别是table的结构识别(行列、单元格)，因此这里汇总了这方面的主要工作。&lt;/p&gt;</summary>
    
    
    
    <category term="OCR" scheme="https://yangsuhui.github.io/categories/OCR/"/>
    
    
    <category term="文档理解" scheme="https://yangsuhui.github.io/tags/%E6%96%87%E6%A1%A3%E7%90%86%E8%A7%A3/"/>
    
  </entry>
  
  <entry>
    <title>OCR(文档预处理--矫正/去噪/畸变/去光照)方法总结</title>
    <link href="https://yangsuhui.github.io/p/5db7.html"/>
    <id>https://yangsuhui.github.io/p/5db7.html</id>
    <published>2021-01-06T12:25:34.000Z</published>
    <updated>2021-01-07T07:33:44.794Z</updated>
    
    <content type="html"><![CDATA[<p>1、unet 实现文本文档去噪、去水印</p><p><a href="https://github.com/1024210879/unet-denoising-dirty-documents">https://github.com/1024210879/unet-denoising-dirty-documents</a></p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106192058.jpg" alt="test_002" style="zoom:50%;" /><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106191852.jpg" alt="test_001" style="zoom:50%;" /><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106192222.jpg" alt="test_003" style="zoom:50%;" /><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106192128.jpg" alt="test_004" style="zoom:50%;" /><p>2、 Distorted Document Images dataset (DDI-100)</p><p><a href="https://github.com/machine-intelligence-laboratory/DDI-100">https://github.com/machine-intelligence-laboratory/DDI-100</a></p><p>imgs</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106192519.jpg" alt="imgs_1"></p><p>masks</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106192542.jpg" alt="masks"></p><p>3、DocProj(Document Rectification and Illumination Correction using a Patch-based CNN: SIGGRAPH Asia 2019)</p><p><a href="https://github.com/xiaoyu258/DocProj">https://github.com/xiaoyu258/DocProj</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106201403.jpg" alt="teaser"></p><p>3-1、GeoProj(Blind Geometric Distortion Correction on Images Through Deep Learning: CVPR19)</p><p><a href="https://github.com/xiaoyu258/GeoProj">https://github.com/xiaoyu258/GeoProj</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106202027.jpg" alt="results"></p><p>4、waveCorrection(复现阿里OCR皱巴巴文档图像形变矫正)</p><p><a href="https://github.com/tommyMessi/waveCorrection">https://github.com/tommyMessi/waveCorrection</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106193321.png" alt="result2"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106193403.png" alt="result2 (1)"><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106193418.png" alt="result3"></p><p>5、DocumentPhotoCorrection(利用unet网络进行 文档照片透视纠偏)</p><p><a href="https://github.com/tommyMessi/DocumentPhotoCorrection">https://github.com/tommyMessi/DocumentPhotoCorrection</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106201350.jpg" alt="i3"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106201255.jpg" alt="r3"></p><p>6、python opencv 文档照片与证件照片的仿射变换的矫正</p><p><a href="https://github.com/tommyMessi/PerspectiveExample">https://github.com/tommyMessi/PerspectiveExample</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106201603.jpeg" alt="jiaozheng" style="zoom:50%;" /><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20210106201621.jpeg" alt="jiaozheng_result" style="zoom:50%;" /></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;通用文档识别精度往往受到用户上传误操作带来的图像质量差(光照不均，形状畸变，污渍噪声、扭曲等)的影响，因此需要一定的图像预处理操作提升模型输入的图像质量。&lt;/p&gt;</summary>
    
    
    
    <category term="OCR" scheme="https://yangsuhui.github.io/categories/OCR/"/>
    
    
    <category term="文档预处理" scheme="https://yangsuhui.github.io/tags/%E6%96%87%E6%A1%A3%E9%A2%84%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>pytorch数据读取优化加速</title>
    <link href="https://yangsuhui.github.io/p/106f.html"/>
    <id>https://yangsuhui.github.io/p/106f.html</id>
    <published>2020-12-25T03:00:15.000Z</published>
    <updated>2021-01-07T07:31:42.275Z</updated>
    
    <content type="html"><![CDATA[<p>1、</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;1、&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>pytorch从单机单卡到单机多卡直至分布式训练</title>
    <link href="https://yangsuhui.github.io/p/b1d2.html"/>
    <id>https://yangsuhui.github.io/p/b1d2.html</id>
    <published>2020-12-25T02:59:53.000Z</published>
    <updated>2021-01-07T07:31:42.273Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>conda/pip/linux软件源替换提高软件下载速度</title>
    <link href="https://yangsuhui.github.io/p/1fd1.html"/>
    <id>https://yangsuhui.github.io/p/1fd1.html</id>
    <published>2020-12-19T09:29:13.000Z</published>
    <updated>2020-12-19T09:59:34.484Z</updated>
    
    <content type="html"><![CDATA[<p>1、.condarc主要用于conda install时的软件源，需要安装conda环境；<br>mv .condarc ~/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">##.condarc</span><br><span class="line">channels:</span><br><span class="line">  - defaults</span><br><span class="line">show_channel_urls: true</span><br><span class="line">channel_alias: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda</span><br><span class="line">default_channels:</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;main</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;r</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;pro</span><br><span class="line">  - https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;msys2</span><br><span class="line">custom_channels:</span><br><span class="line">  conda-forge: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  msys2: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  bioconda: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  menpo: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  pytorch: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br><span class="line">  simpleitk: https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;cloud</span><br></pre></td></tr></table></figure><p>2、pip.conf主要指定pip install时软件源地址；<br>mkdir -p ~/.pip<br>mv pip.conf ~/.pip</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">##pip.conf</span><br><span class="line">[global]</span><br><span class="line">index-url &#x3D; http:&#x2F;&#x2F;pypi.douban.com&#x2F;simple</span><br><span class="line">extra-index-url &#x3D; https:&#x2F;&#x2F;pypi.tuna.tsinghua.edu.cn&#x2F;simple</span><br><span class="line">trusted-host &#x3D;</span><br><span class="line">    pypi.douban.com</span><br><span class="line">    pypi.tuna.tsinghua</span><br><span class="line">timeout &#x3D; 120</span><br></pre></td></tr></table></figure><p>3、sources.list主要指定ubuntu系统apt-get install时的软件源，更新sources.list后需要apt update一下才生效。<br>mv /etc/apt/sources.list /etc/apt/sources_bck.list<br>mv sources.list /etc/apt/</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">##sources.list</span><br><span class="line">deb-src http:&#x2F;&#x2F;archive.ubuntu.com&#x2F;ubuntu xenial main restricted #Added by software-properties</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial main restricted</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates main restricted</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial universe</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates universe</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-updates multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-backports main restricted universe multiverse</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-backports main restricted universe multiverse #Added by software-properties</span><br><span class="line">deb http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partner</span><br><span class="line">deb-src http:&#x2F;&#x2F;archive.canonical.com&#x2F;ubuntu xenial partner</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security main restricted</span><br><span class="line">deb-src http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security main restricted multiverse universe #Added by software-properties</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security universe</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ubuntu&#x2F; xenial-security multiverse</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要提供好用的软件源地址配置。&lt;/p&gt;</summary>
    
    
    
    <category term="开发环境配置" scheme="https://yangsuhui.github.io/categories/%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE/"/>
    
    
    <category term="实用手册" scheme="https://yangsuhui.github.io/tags/%E5%AE%9E%E7%94%A8%E6%89%8B%E5%86%8C/"/>
    
  </entry>
  
  <entry>
    <title>Trtserver(Triton)使用示例</title>
    <link href="https://yangsuhui.github.io/p/17b9.html"/>
    <id>https://yangsuhui.github.io/p/17b9.html</id>
    <published>2020-12-18T11:44:45.000Z</published>
    <updated>2020-12-22T09:46:27.790Z</updated>
    
    <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><h1 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h1><p>1、生成TensorRT的模型engine，如果新的网络有些层TensorRT不支持，还需要写这些特定层的plugin，转换好后，可以直接用TensorRT测试下；<br>2、通过Trtserver(TensorRT Inference Server)，现在叫TriTon调用第一步生成的model引擎；<br>3、安装有trtserver-clientSDK的客户端调用模型进行推理；<br><strong>注意</strong>: 模型转TensorRT时，如果是pytorch–&gt;&gt;ONNX–&gt;&gt;trt, tensorflow(.pb)–&gt;uff–&gt;&gt;trt或者ckpt–&gt;&gt;ONNX–&gt;&gt;trt;<br>2、3部是trtserver调用模型开启服务，轮询指定端口，客户端调用，这种方式和tf-serving部署pb模型方式类似；<br>其他部署方式：(libtorch、torchscript)或者RestfulAPI形式(fastapi/flask进行调用模型，开通服务端口)<br>上述部署方式都可以进一步结合k8s和docker，部署成微服务的形式。<br>思考：trtserver主要用于资源调度层面的(更加上层)，TensorRT主要用于模型的推理加速方面，因此trtserver更像是tf-serving的功能；<br>在模型转TensorRT之前，可以进一步采用量化、剪枝等技术实现模型压缩；</p><h1 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h1><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://github.com/isarsoft/yolov4-triton-tensorrt">https://github.com/isarsoft/yolov4-triton-tensorrt</a><br><a href="https://github.com/layerism/TensorRT-Inference-Server-Tutorial">https://github.com/layerism/TensorRT-Inference-Server-Tutorial</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要介绍如何搭建Microsoft的NNI工具环境以及使用NNI进行Mnist分类任务的超参数搜索。&lt;/p&gt;</summary>
    
    
    
    <category term="模型部署" scheme="https://yangsuhui.github.io/categories/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="模型部署" scheme="https://yangsuhui.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"/>
    
  </entry>
  
  <entry>
    <title>TensorRT安装并使用简述</title>
    <link href="https://yangsuhui.github.io/p/4074.html"/>
    <id>https://yangsuhui.github.io/p/4074.html</id>
    <published>2020-12-17T11:12:21.000Z</published>
    <updated>2020-12-17T15:06:20.792Z</updated>
    
    <content type="html"><![CDATA[<h1 id="环境安装"><a href="#环境安装" class="headerlink" title="环境安装"></a>环境安装</h1><p>我这里使用的是TensorRT-5.1.5.0版本，其他版本可能会有一些出入。<br>安装好的TensorRT环境的docker镜像(docker pull 857470845/hvd_trt_apex_torch:v1)可供下载使用。</p><h1 id="示例运行"><a href="#示例运行" class="headerlink" title="示例运行"></a>示例运行</h1><p>主要测试两种示例：1、利用安装好的convert_to_uff.py脚本将tensorflow的pb模型文件(lenet)转成uff格式文件，并测试mnist数据；<br>将TensorRT-5.1.5.0项目所在路径挂载到/mnt下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">##cd &#x2F;mnt&#x2F;TensorRT-5.1.5.0&#x2F;samples&#x2F;python&#x2F;end_to_end_tensorflow_mnist</span><br><span class="line">mkdir models</span><br><span class="line">python model.py</span><br><span class="line">##convert_to_uff.py脚本在镜像中的路径：&#x2F;opt&#x2F;conda&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;uff&#x2F;bin&#x2F;convert_to_uff.py</span><br><span class="line">python &#x2F;opt&#x2F;conda&#x2F;lib&#x2F;python3.6&#x2F;site-packages&#x2F;uff&#x2F;bin&#x2F;convert_to_uff.py --input_file models&#x2F;lenet5.pb</span><br></pre></td></tr></table></figure><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20201217223242.JPG" alt="trt_1"></p><p>2、直接测试samples/sampleMNIST程序<br>可以直接在samples下make，产生所有的示例的bin；不过我这里是只单独编译测试samples/sampleMNIST示例。</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20201217223757.JPG" alt="trt-2"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20201217223939.JPG" alt="trt"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">root@gpuserver002:&#x2F;mnt&#x2F;TensorRT-5.1.5.0&#x2F;bin# .&#x2F;sample_mnist --int8</span><br><span class="line">&amp;&amp;&amp;&amp; RUNNING TensorRT.sample_mnist # .&#x2F;sample_mnist --int8</span><br><span class="line">[I] Building and running a GPU inference engine for MNIST</span><br><span class="line">[W] [TRT] Calibrator is not being used. Users must provide dynamic range for all tensors that are not Int32.</span><br><span class="line">[W] [TRT] Warning: no implementation of (Unnamed Layer* 9) [Constant] obeys the requested constraints, using a higher precision type</span><br><span class="line">[W] [TRT] Warning: no implementation of ip2 obeys the requested constraints, using a higher precision type</span><br><span class="line">[W] [TRT] Warning: no implementation of prob obeys the requested constraints, using a higher precision type</span><br><span class="line">[I] Input:</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@%+-:  &#x3D;@@@@@@@@@@@@</span><br><span class="line">@@@@@@@%&#x3D;      -@@@**@@@@@@@</span><br><span class="line">@@@@@@@   :%#@-#@@@. #@@@@@@</span><br><span class="line">@@@@@@*  +@@@@:*@@@  *@@@@@@</span><br><span class="line">@@@@@@#  +@@@@ @@@%  @@@@@@@</span><br><span class="line">@@@@@@@.  :%@@.@@@. *@@@@@@@</span><br><span class="line">@@@@@@@@-   &#x3D;@@@@. -@@@@@@@@</span><br><span class="line">@@@@@@@@@%:   +@- :@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@%.  : -@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@+   #@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@+  :@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@+   *@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@: &#x3D;  @@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@ :@  @@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@ -@  @@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@# +@  @@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@* ++  @@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@*    *@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@#   &#x3D;@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@. +@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line">@@@@@@@@@@@@@@@@@@@@@@@@@@@@</span><br><span class="line"></span><br><span class="line">[I] Output:</span><br><span class="line">0:</span><br><span class="line">1:</span><br><span class="line">2:</span><br><span class="line">3:</span><br><span class="line">4:</span><br><span class="line">5:</span><br><span class="line">6:</span><br><span class="line">7:</span><br><span class="line">8: **********</span><br><span class="line">9:</span><br><span class="line"></span><br><span class="line">&amp;&amp;&amp;&amp; PASSED TensorRT.sample_mnist # .&#x2F;sample_mnist --int8</span><br></pre></td></tr></table></figure><p>推荐工程：<br><a href="https://github.com/NVIDIA/object-detection-tensorrt-example">https://github.com/NVIDIA/object-detection-tensorrt-example</a><br><a href="https://github.com/NVIDIA/healthcare-on-tap-TRT-TRITON-demo">https://github.com/NVIDIA/healthcare-on-tap-TRT-TRITON-demo</a></p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>下载链接: <a href="https://developer.nvidia.com/nvidia-tensorrt-download">https://developer.nvidia.com/nvidia-tensorrt-download</a><br>trt工程: <a href="https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html">https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html</a><br>项目地址: <a href="https://github.com/NVIDIA/TensorRT">https://github.com/NVIDIA/TensorRT</a><br>参考博客: <a href="https://blog.csdn.net/zong596568821xp/article/details/86077553">https://blog.csdn.net/zong596568821xp/article/details/86077553</a><br>Triton(tensorrt inference server): <a href="https://github.com/triton-inference-server/server">https://github.com/triton-inference-server/server</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要简要介绍TensorRT的环境搭建及如何使用。&lt;/p&gt;</summary>
    
    
    
    <category term="TensorRT" scheme="https://yangsuhui.github.io/categories/TensorRT/"/>
    
    
    <category term="模型部署" scheme="https://yangsuhui.github.io/tags/%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2/"/>
    
    <category term="TensorRT" scheme="https://yangsuhui.github.io/tags/TensorRT/"/>
    
  </entry>
  
  <entry>
    <title>自动超参搜索之NNI工具使用</title>
    <link href="https://yangsuhui.github.io/p/b5ee.html"/>
    <id>https://yangsuhui.github.io/p/b5ee.html</id>
    <published>2020-12-16T09:45:13.000Z</published>
    <updated>2020-12-17T08:53:50.987Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NNI简介"><a href="#NNI简介" class="headerlink" title="NNI简介"></a>NNI简介</h1><p>NNI (Neural Network Intelligence) 是一个轻量但强大的工具包，帮助用户自动的进行特征工程，神经网络架构搜索，超参调优以及模型压缩。<br>NNI 管理自动机器学习 (AutoML) 的 Experiment，调度运行由调优算法生成的 Trial 任务来找到最好的神经网络架构和/或超参，支持各种训练环境，如本机，远程服务器，OpenPAI，Kubeflow，基于 K8S 的 FrameworkController（如，AKS 等)， DLWorkspace (又称 DLTS), AML (Azure Machine Learning) 以及其它环境。</p><h1 id="NNI环境安装"><a href="#NNI环境安装" class="headerlink" title="NNI环境安装"></a>NNI环境安装</h1><p>根据官网<a href="https://github.com/microsoft/nni/blob/master/README_zh_CN.md">安装教程</a>进行安装即可，也可以直接下载已经安装好NNI环境的docker镜像(docker pull 857470845/hvd_trt_apex_torch:v1)进行测试。<br>说明：NNI工程download下来后，deployment文件夹下有pypi和docker两种安装方式；docs文件夹下是一些使用说明保护config配置文件中的一些参数含义；examples下是<br>一些具体示例。</p><h1 id="测试MNIST-pytorch"><a href="#测试MNIST-pytorch" class="headerlink" title="测试MNIST-pytorch"></a>测试MNIST-pytorch</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nnictl create --config nni&#x2F;examples&#x2F;trials&#x2F;mnist-tfv1&#x2F;config.yml -p 8888</span><br></pre></td></tr></table></figure><p>-p 指定运行端口；</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20201217153623.JPG" alt="nni"><br>本地chrome浏览器打开<a href="http://172.22.22.203:8888/%EF%BC%8C%E7%BB%93%E6%9E%9C%E5%A6%82%E4%B8%8B%EF%BC%9A">http://172.22.22.203:8888/，结果如下：</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20201217154204.JPG" alt="nni-client"></p><p>超参数搜索的结果：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20201217155358.JPG" alt="nni-result"></p><p>对应的accuracy：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20201217155804.JPG" alt="nni-metric"></p><p>同样的方式运行pytorch版本的MNIST:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nnictl create --config nni&#x2F;examples&#x2F;trials&#x2F;mnist-pytorch&#x2F;config.yml -p 8888</span><br></pre></td></tr></table></figure><p><strong>不过直接从官网下载的mnist-pytorch下的config.yml有一些<a href="https://github.com/microsoft/nni/blob/master/docs/zh_CN/Tutorial/ExperimentConfig.md">参数</a>需要修改一下才能使用GPU训练；</strong><br>这是我修改后的config.yml配置文件，仅供参考。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">authorName: default</span><br><span class="line">experimentName: example_mnist_pytorch</span><br><span class="line">trialConcurrency: 2</span><br><span class="line">maxExecDuration: 1h</span><br><span class="line">maxTrialNum: 10</span><br><span class="line">#choice: local, remote, pai</span><br><span class="line">trainingServicePlatform: local</span><br><span class="line">searchSpacePath: search_space.json</span><br><span class="line">#choice: true, false</span><br><span class="line">useAnnotation: false</span><br><span class="line">tuner:</span><br><span class="line">  #choice: TPE, Random, Anneal, Evolution, BatchTuner, MetisTuner, GPTuner</span><br><span class="line">  #SMAC (SMAC should be installed through nnictl)</span><br><span class="line">  builtinTunerName: TPE</span><br><span class="line">  classArgs:</span><br><span class="line">    #choice: maximize, minimize</span><br><span class="line">    optimize_mode: maximize</span><br><span class="line">trial:</span><br><span class="line">  command: python3 mnist.py</span><br><span class="line">  codeDir: .</span><br><span class="line">  gpuNum: 2</span><br><span class="line">localConfig:</span><br><span class="line">  maxTrialNumPerGpu:  2</span><br><span class="line">  useActiveGpu: true</span><br><span class="line">  gpuIndices: 1,3</span><br></pre></td></tr></table></figure><p><strong><em>特别说明</em></strong>：NNI可以本地运行，也可以和其他框架进行插件结合<a href="https://nni.readthedocs.io/en/latest/Tutorial/QuickStart.html#related-topic">配置</a>，比如和Microsoft的OpenPAI平台结合，就可以将训练任务上传到PAI平台上进行NNI训练，<br>不同的配置方式使用不同的config文件；</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p>NNI项目地址: <a href="https://github.com/microsoft/nni/blob/master/README_zh_CN.md">https://github.com/microsoft/nni/blob/master/README_zh_CN.md</a><br>使用说明: <a href="https://nni.readthedocs.io/en/latest/Tutorial/QuickStart.html">https://nni.readthedocs.io/en/latest/Tutorial/QuickStart.html</a><br>config参数: <a href="https://github.com/microsoft/nni/blob/master/docs/zh_CN/Tutorial/ExperimentConfig.md">https://github.com/microsoft/nni/blob/master/docs/zh_CN/Tutorial/ExperimentConfig.md</a><br>推荐参考: <a href="https://blog.csdn.net/weixin_43653494/article/details/101039198">https://blog.csdn.net/weixin_43653494/article/details/101039198</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要介绍如何搭建Microsoft的NNI工具环境以及使用NNI进行Mnist分类任务的超参数搜索。&lt;/p&gt;</summary>
    
    
    
    <category term="AutoML" scheme="https://yangsuhui.github.io/categories/AutoML/"/>
    
    
    <category term="AutoML" scheme="https://yangsuhui.github.io/tags/AutoML/"/>
    
  </entry>
  
  <entry>
    <title>COCOAPI 评价指标解析及功能改进</title>
    <link href="https://yangsuhui.github.io/p/5b87.html"/>
    <id>https://yangsuhui.github.io/p/5b87.html</id>
    <published>2020-12-15T03:43:27.000Z</published>
    <updated>2021-01-11T07:36:33.575Z</updated>
    
    <content type="html"><![CDATA[<h1 id="程序入口"><a href="#程序入口" class="headerlink" title="程序入口"></a>程序入口</h1><p>python eval_coco.py</p><h2 id="特别说明"><a href="#特别说明" class="headerlink" title="特别说明"></a>特别说明</h2><p>results_test.json格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;image_id&quot;: 19, &quot;category_id&quot;: 1, &quot;bbox&quot;: [121.4, 116.02, 560.56, 303.83], &quot;score&quot;: 0.97&#125;, &#123;&quot;image_id&quot;: 19, &quot;category_id&quot;: 1, &quot;bbox&quot;: [119.3, 748.22, 566.03, 267.83], &quot;score&quot;: 0.95&#125;, .......</span><br><span class="line">&#123;&quot;image_id&quot;: 320, &quot;category_id&quot;: 3, &quot;bbox&quot;: [329.74, 992.53, 35.72, 9.86], &quot;score&quot;: 0.62&#125;]</span><br></pre></td></tr></table></figure><p>其中image_id和category_id和instances_test.json中的保持一致，而instances_test.json就是标准的coco格式的gt文件。具体格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;licenses&quot;: [&#123;&quot;name&quot;: &quot;&quot;, &quot;id&quot;: 0, &quot;url&quot;: &quot;&quot;&#125;], &quot;info&quot;: &#123;&quot;contributor&quot;: &quot;&quot;, &quot;date_created&quot;: &quot;2020-11-16&quot;, &quot;description&quot;: &quot;table_parse_second_public&quot;, &quot;url&quot;: &quot;&quot;, &quot;version&quot;: 2, &quot;year&quot;: &quot;2020&quot;&#125;, &quot;categories&quot;: [&#123;&quot;id&quot;: 1, &quot;name&quot;: &quot;bordered&quot;, &quot;supercategory&quot;: &quot;&quot;&#125;, &#123;&quot;id&quot;: 2, &quot;name&quot;: &quot;borderless&quot;, &quot;supercategory&quot;: &quot;&quot;&#125;, &#123;&quot;id&quot;: 3, &quot;name&quot;: &quot;cell&quot;, &quot;supercategory&quot;: &quot;&quot;&#125;], &quot;images&quot;: [&#123;&quot;coco_url&quot;: &quot;&quot;, &quot;date_captured&quot;: &quot;&quot;, &quot;flickr_url&quot;: &quot;&quot;, &quot;license&quot;: 0, &quot;id&quot;: 19, &quot;file_name&quot;: &quot;cTDaR_t10019.jpg&quot;, &quot;height&quot;: 1123, &quot;width&quot;: 794&#125;, &#123;&quot;coco_url&quot;: &quot;&quot;, &quot;date_captured&quot;: &quot;&quot;, &quot;flickr_url&quot;: &quot;&quot;, &quot;license&quot;: 0, &quot;id&quot;: 21, &quot;file_name&quot;: &quot;cTDaR_t10021.jpg&quot;, &quot;height&quot;: 1059, &quot;width&quot;: 794&#125;, &#123;&quot;coco_url&quot;: &quot;&quot;, &quot;date_captured&quot;: &quot;&quot;, &quot;flickr_url&quot;: &quot;&quot;, &quot;license&quot;: 0, &quot;id&quot;: 320, &quot;file_name&quot;: &quot;cTDaR_t10507.jpg&quot;, &quot;height&quot;: 1056, &quot;width&quot;: 816&#125;],&quot;annotations&quot;: [&#123;&quot;category_id&quot;: 1, &quot;id&quot;: 1218, &quot;image_id&quot;: 19, &quot;iscrowd&quot;: 0, &quot;segmentation&quot;: [[110.0, 96.0, 683.0, 96.0, 683.0, 437.0, 110.0, 437.0]], &quot;area&quot;: 195393.0, &quot;bbox&quot;: [110.0, 96.0, 573.0, 341.0]&#125;, &#123;&quot;category_id&quot;: 1, &quot;id&quot;: 1219, &quot;image_id&quot;: 19, &quot;iscrowd&quot;: 0, &quot;segmentation&quot;: [[110.0, 732.0, 683.0, 732.0, 683.0, 1025.0, 110.0, 1025.0]], &quot;area&quot;: 167889.0, &quot;bbox&quot;: [110.0, 732.0, 573.0, 293.0]&#125;,&#123;&quot;category_id&quot;: 3, &quot;id&quot;: 21911, &quot;image_id&quot;: 320, &quot;iscrowd&quot;: 0, &quot;segmentation&quot;: [[416.0, 845.0, 428.0, 845.0, 428.0, 855.0, 416.0, 855.0]], &quot;area&quot;: 120.0, &quot;bbox&quot;: [416.0, 845.0, 12.0, 10.0]&#125;]&#125;</span><br></pre></td></tr></table></figure><h2 id="eval-coco-py"><a href="#eval-coco-py" class="headerlink" title="eval_coco.py"></a>eval_coco.py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">##The code of eval_coco.py</span><br><span class="line">import pycocotools.coco as coco</span><br><span class="line">from pycocotools.cocoeval import COCOeval</span><br><span class="line">results &#x3D; r&#39;.&#x2F;results_test.json&#39;  ##模型预测结果</span><br><span class="line">anno &#x3D; r&#39;.&#x2F;instances_test2017.json&#39;  ##ground truth</span><br><span class="line">coco_anno &#x3D; coco.COCO(anno)</span><br><span class="line">coco_dets &#x3D; coco_anno.loadRes(results)</span><br><span class="line">coco_eval &#x3D; COCOeval(coco_anno, coco_dets, &quot;bbox&quot;)</span><br><span class="line">coco_eval.evaluate()</span><br><span class="line">coco_eval.accumulate()</span><br><span class="line">coco_eval.summarize()</span><br><span class="line">coco_eval.get_good_predict_data()</span><br></pre></td></tr></table></figure><h2 id="修改后的cocoeval-py"><a href="#修改后的cocoeval-py" class="headerlink" title="修改后的cocoeval.py"></a>修改后的cocoeval.py</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br><span class="line">771</span><br><span class="line">772</span><br><span class="line">773</span><br><span class="line">774</span><br><span class="line">775</span><br><span class="line">776</span><br><span class="line">777</span><br><span class="line">778</span><br><span class="line">779</span><br><span class="line">780</span><br><span class="line">781</span><br><span class="line">782</span><br><span class="line">783</span><br><span class="line">784</span><br><span class="line">785</span><br><span class="line">786</span><br><span class="line">787</span><br><span class="line">788</span><br><span class="line">789</span><br><span class="line">790</span><br><span class="line">791</span><br><span class="line">792</span><br><span class="line">793</span><br><span class="line">794</span><br><span class="line">795</span><br><span class="line">796</span><br><span class="line">797</span><br><span class="line">798</span><br><span class="line">799</span><br><span class="line">800</span><br><span class="line">801</span><br><span class="line">802</span><br><span class="line">803</span><br><span class="line">804</span><br><span class="line">805</span><br><span class="line">806</span><br><span class="line">807</span><br><span class="line">808</span><br><span class="line">809</span><br><span class="line">810</span><br><span class="line">811</span><br><span class="line">812</span><br><span class="line">813</span><br><span class="line">814</span><br><span class="line">815</span><br><span class="line">816</span><br><span class="line">817</span><br><span class="line">818</span><br><span class="line">819</span><br><span class="line">820</span><br></pre></td><td class="code"><pre><span class="line">__author__ &#x3D; &#39;tsungyi_ysh&#39;</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import datetime</span><br><span class="line">import time</span><br><span class="line">from collections import defaultdict</span><br><span class="line">from . import mask as maskUtils</span><br><span class="line">import copy</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">class COCOeval:</span><br><span class="line">    # Interface for evaluating detection on the Microsoft COCO dataset.</span><br><span class="line">    #</span><br><span class="line">    # The usage for CocoEval is as follows:</span><br><span class="line">    #  cocoGt&#x3D;..., cocoDt&#x3D;...       # load dataset and results</span><br><span class="line">    #  E &#x3D; CocoEval(cocoGt,cocoDt); # initialize CocoEval object</span><br><span class="line">    #  E.params.recThrs &#x3D; ...;      # set parameters as desired</span><br><span class="line">    #  E.evaluate();                # run per image evaluation</span><br><span class="line">    #  E.accumulate();              # accumulate per image results</span><br><span class="line">    #  E.summarize();               # display summary metrics of results</span><br><span class="line">    # For example usage see evalDemo.m and http:&#x2F;&#x2F;mscoco.org&#x2F;.</span><br><span class="line">    #</span><br><span class="line">    # The evaluation parameters are as follows (defaults in brackets):</span><br><span class="line">    #  imgIds     - [all] N img ids to use for evaluation</span><br><span class="line">    #  catIds     - [all] K cat ids to use for evaluation</span><br><span class="line">    #  iouThrs    - [.5:.05:.95] T&#x3D;10 IoU thresholds for evaluation</span><br><span class="line">    #  recThrs    - [0:.01:1] R&#x3D;101 recall thresholds for evaluation</span><br><span class="line">    #  areaRng    - [...] A&#x3D;4 object area ranges for evaluation</span><br><span class="line">    #  maxDets    - [1 10 100] M&#x3D;3 thresholds on max detections per image</span><br><span class="line">    #  iouType    - [&#39;segm&#39;] set iouType to &#39;segm&#39;, &#39;bbox&#39; or &#39;keypoints&#39;</span><br><span class="line">    #  iouType replaced the now DEPRECATED useSegm parameter.</span><br><span class="line">    #  useCats    - [1] if true use category labels for evaluation</span><br><span class="line">    # Note: if useCats&#x3D;0 category labels are ignored as in proposal scoring.</span><br><span class="line">    # Note: multiple areaRngs [Ax2] and maxDets [Mx1] can be specified.</span><br><span class="line">    #</span><br><span class="line">    # evaluate(): evaluates detections on every image and every category and</span><br><span class="line">    # concats the results into the &quot;evalImgs&quot; with fields:</span><br><span class="line">    #  dtIds      - [1xD] id for each of the D detections (dt)</span><br><span class="line">    #  gtIds      - [1xG] id for each of the G ground truths (gt)</span><br><span class="line">    #  dtMatches  - [TxD] matching gt id at each IoU or 0</span><br><span class="line">    #  gtMatches  - [TxG] matching dt id at each IoU or 0</span><br><span class="line">    #  dtScores   - [1xD] confidence of each dt</span><br><span class="line">    #  gtIgnore   - [1xG] ignore flag for each gt</span><br><span class="line">    #  dtIgnore   - [TxD] ignore flag for each dt at each IoU</span><br><span class="line">    #</span><br><span class="line">    # accumulate(): accumulates the per-image, per-category evaluation</span><br><span class="line">    # results in &quot;evalImgs&quot; into the dictionary &quot;eval&quot; with fields:</span><br><span class="line">    #  params     - parameters used for evaluation</span><br><span class="line">    #  date       - date evaluation was performed</span><br><span class="line">    #  counts     - [T,R,K,A,M] parameter dimensions (see above)</span><br><span class="line">    #  precision  - [TxRxKxAxM] precision for every evaluation setting</span><br><span class="line">    #  recall     - [TxKxAxM] max recall for every evaluation setting</span><br><span class="line">    # Note: precision and recall&#x3D;&#x3D;-1 for settings with no gt objects.</span><br><span class="line">    #</span><br><span class="line">    # See also coco, mask, pycocoDemo, pycocoEvalDemo</span><br><span class="line">    #</span><br><span class="line">    # Microsoft COCO Toolbox.      version 2.0</span><br><span class="line">    # Data, paper, and tutorials available at:  http:&#x2F;&#x2F;mscoco.org&#x2F;</span><br><span class="line">    # Code written by Piotr Dollar and Tsung-Yi Lin, 2015.</span><br><span class="line">    # Licensed under the Simplified BSD License [see coco&#x2F;license.txt]</span><br><span class="line">    def __init__(self, cocoGt&#x3D;None, cocoDt&#x3D;None, iouType&#x3D;&#39;segm&#39;):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        Initialize CocoEval using coco APIs for gt and dt</span><br><span class="line">        :param cocoGt: coco object with ground truth annotations</span><br><span class="line">        :param cocoDt: coco object with detection results</span><br><span class="line">        :return: None</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        if not iouType:</span><br><span class="line">            print(&#39;iouType not specified. use default iouType segm&#39;)</span><br><span class="line">        self.cocoGt   &#x3D; cocoGt              # ground truth COCO API</span><br><span class="line">        self.cocoDt   &#x3D; cocoDt              # detections COCO API</span><br><span class="line">        self.evalImgs &#x3D; defaultdict(list)   # per-image per-category evaluation results [KxAxI] elements</span><br><span class="line">        self.eval     &#x3D; &#123;&#125;                  # accumulated evaluation results</span><br><span class="line">        self._gts &#x3D; defaultdict(list)       # gt for evaluation</span><br><span class="line">        self._dts &#x3D; defaultdict(list)       # dt for evaluation</span><br><span class="line">        self.params &#x3D; Params(iouType&#x3D;iouType) # parameters</span><br><span class="line">        self._paramsEval &#x3D; &#123;&#125;               # parameters for evaluation</span><br><span class="line">        self.stats &#x3D; []                     # result summarization</span><br><span class="line">        self.ious &#x3D; &#123;&#125;                      # ious between all gts and dts</span><br><span class="line">        print(&#39;-----&#39;)</span><br><span class="line">        if not cocoGt is None:</span><br><span class="line">            self.params.imgIds &#x3D; sorted(cocoGt.getImgIds())</span><br><span class="line">            self.params.catIds &#x3D; sorted(cocoGt.getCatIds())</span><br><span class="line">            print(&#39;length of self.params.imgIds:&#39;,len(self.params.imgIds))</span><br><span class="line">            print(&#39;self.params.catIds:&#39;,self.params.catIds)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    def _prepare(self):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        Prepare ._gts and ._dts for evaluation based on params</span><br><span class="line">        :return: None</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        def _toMask(anns, coco):</span><br><span class="line">            # modify ann[&#39;segmentation&#39;] by reference</span><br><span class="line">            for ann in anns:</span><br><span class="line">                rle &#x3D; coco.annToRLE(ann)</span><br><span class="line">                ann[&#39;segmentation&#39;] &#x3D; rle</span><br><span class="line">        p &#x3D; self.params</span><br><span class="line"></span><br><span class="line">        ##通过查看保存的hk_noline检测的json，gts是单幅图像100个检测框，类别都是1(因为hk_noline只有一个类别)</span><br><span class="line">        ##gt是一幅图像对应的gt框，这里的hk_noline是单类别，所以useCats是0，是1，保存的json内容都是一样的</span><br><span class="line">        ##具体的gts和dts的json格式是一个列表，每一个元素是一个字典，一个字典是一个检测框信息；</span><br><span class="line">        ##进一步测试下多类的情况，不同的useCats效果？？</span><br><span class="line">        if p.useCats:</span><br><span class="line">            gts&#x3D;self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds&#x3D;p.imgIds, catIds&#x3D;p.catIds))</span><br><span class="line">            dts&#x3D;self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds&#x3D;p.imgIds, catIds&#x3D;p.catIds))</span><br><span class="line">            f_gts &#x3D; open(&#39;.&#x2F;tmp1214&#x2F;gts_catid.json&#39;,&#39;w+&#39;)</span><br><span class="line">            json_gt &#x3D; json.dumps(gts)</span><br><span class="line">            f_gts.write(json_gt)</span><br><span class="line">            f_gts.close()</span><br><span class="line"></span><br><span class="line">            f_dts &#x3D; open(&#39;.&#x2F;tmp1214&#x2F;dts_catid.json&#39;,&#39;w+&#39;)</span><br><span class="line">            json_dt &#x3D; json.dumps(dts)</span><br><span class="line">            f_dts.write(json_dt)</span><br><span class="line">            f_dts.close()</span><br><span class="line"></span><br><span class="line">            #print(&#39;gts:&#39;,gts)</span><br><span class="line">            #print(&#39;dts:&#39;,dts)</span><br><span class="line">        else:</span><br><span class="line">            gts&#x3D;self.cocoGt.loadAnns(self.cocoGt.getAnnIds(imgIds&#x3D;p.imgIds))</span><br><span class="line">            dts&#x3D;self.cocoDt.loadAnns(self.cocoDt.getAnnIds(imgIds&#x3D;p.imgIds))</span><br><span class="line"></span><br><span class="line">            f_gts &#x3D; open(&#39;.&#x2F;tmp1214&#x2F;gts_no_catid.json&#39;,&#39;w+&#39;)</span><br><span class="line">            json_gt &#x3D; json.dumps(gts)</span><br><span class="line">            f_gts.write(json_gt)</span><br><span class="line">            f_gts.close()</span><br><span class="line"></span><br><span class="line">            f_dts &#x3D; open(&#39;.&#x2F;tmp1214&#x2F;dts_no_catid.json&#39;,&#39;w+&#39;)</span><br><span class="line">            json_dt &#x3D; json.dumps(dts)</span><br><span class="line">            f_dts.write(json_dt)</span><br><span class="line">            f_dts.close()</span><br><span class="line"></span><br><span class="line">        # convert ground truth to mask if iouType &#x3D;&#x3D; &#39;segm&#39;</span><br><span class="line">        if p.iouType &#x3D;&#x3D; &#39;segm&#39;:</span><br><span class="line">            _toMask(gts, self.cocoGt)</span><br><span class="line">            _toMask(dts, self.cocoDt)</span><br><span class="line">        # set ignore flag</span><br><span class="line">        for gt in gts:</span><br><span class="line">            gt[&#39;ignore&#39;] &#x3D; gt[&#39;ignore&#39;] if &#39;ignore&#39; in gt else 0</span><br><span class="line">            gt[&#39;ignore&#39;] &#x3D; &#39;iscrowd&#39; in gt and gt[&#39;iscrowd&#39;]</span><br><span class="line">            if p.iouType &#x3D;&#x3D; &#39;keypoints&#39;:</span><br><span class="line">                gt[&#39;ignore&#39;] &#x3D; (gt[&#39;num_keypoints&#39;] &#x3D;&#x3D; 0) or gt[&#39;ignore&#39;]</span><br><span class="line">        ##这种声明方式产生的self._gts是一个字典，每个元素是列表</span><br><span class="line">        ##这样得到的就是相同的img_id和类别id的信息，存放在一个列表中，即一张图像的同一个类别的框在一个列表中；</span><br><span class="line">        self._gts &#x3D; defaultdict(list)       # gt for evaluation</span><br><span class="line">        self._dts &#x3D; defaultdict(list)       # dt for evaluation</span><br><span class="line">        ## gts中一个gt格式：&#123;&quot;area&quot;: 735345, &quot;iscrowd&quot;: 0, &quot;image_id&quot;: 20190000781, &quot;bbox&quot;: [225, 1052, 1257, 585], &quot;category_id&quot;: 1, &quot;id&quot;: 1063, &quot;ignore&quot;: 0, &quot;segmentation&quot;: []&#125;</span><br><span class="line">        for gt in gts:</span><br><span class="line">            self._gts[gt[&#39;image_id&#39;], gt[&#39;category_id&#39;]].append(gt)</span><br><span class="line">        ## dts中一个dt格式：&#123;&quot;image_id&quot;: 20190000781, &quot;category_id&quot;: 1, &quot;bbox&quot;: [1584.88, 884.44, 152.43, 308.34], &quot;score&quot;: 0.0, &quot;segmentation&quot;: [[1584.88, 884.44, 1584.88, 1192.78, 1737.3100000000002, 1192.78, 1737.3100000000002, 884.44]], &quot;area&quot;: 47000.2662, &quot;id&quot;: 78100, &quot;iscrowd&quot;: 0&#125;</span><br><span class="line">        for dt in dts:</span><br><span class="line">            self._dts[dt[&#39;image_id&#39;], dt[&#39;category_id&#39;]].append(dt)</span><br><span class="line">        self.evalImgs &#x3D; defaultdict(list)   # per-image per-category evaluation results</span><br><span class="line">        self.eval     &#x3D; &#123;&#125;                  # accumulated evaluation results</span><br><span class="line"></span><br><span class="line">        self.gt_id2img_id &#x3D; &#123;&#125;</span><br><span class="line">        for gt_i in gts:</span><br><span class="line">            self.gt_id2img_id[gt_i[&#39;id&#39;]] &#x3D; gt_i[&#39;image_id&#39;]</span><br><span class="line">        </span><br><span class="line">        self.gt_imgid_cat_id &#x3D; &#123;&#125;</span><br><span class="line">        for gt_i in gts:</span><br><span class="line">            if gt_i[&#39;image_id&#39;] not in self.gt_imgid_cat_id.keys():</span><br><span class="line">                self.gt_imgid_cat_id[gt_i[&#39;image_id&#39;]] &#x3D; &#123;&#125;</span><br><span class="line">                for cat in self.params.catIds:</span><br><span class="line">                    self.gt_imgid_cat_id[gt_i[&#39;image_id&#39;]][cat] &#x3D; []</span><br><span class="line">            self.gt_imgid_cat_id[gt_i[&#39;image_id&#39;]][gt_i[&#39;category_id&#39;]].append(gt_i[&#39;id&#39;])</span><br><span class="line">                #self.gt_imgid_cat_id[gt_i[&#39;image_id&#39;]][gt_i[&#39;category_id&#39;]].append(gt_i[&#39;id&#39;])</span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">    def evaluate(self):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        Run per image evaluation on given images and store results (a list of dict) in self.evalImgs</span><br><span class="line">        :return: None</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        tic &#x3D; time.time()</span><br><span class="line">        print(&#39;Running per image evaluation...&#39;)</span><br><span class="line">        p &#x3D; self.params</span><br><span class="line">        # add backward compatibility if useSegm is specified in params</span><br><span class="line">        if not p.useSegm is None:</span><br><span class="line">            p.iouType &#x3D; &#39;segm&#39; if p.useSegm &#x3D;&#x3D; 1 else &#39;bbox&#39;</span><br><span class="line">            print(&#39;useSegm (deprecated) is not None. Running &#123;&#125; evaluation&#39;.format(p.iouType))</span><br><span class="line">        print(&#39;Evaluate annotation type *&#123;&#125;*&#39;.format(p.iouType))</span><br><span class="line">        p.imgIds &#x3D; list(np.unique(p.imgIds))  ##唯一imgid</span><br><span class="line">        if p.useCats:</span><br><span class="line">            p.catIds &#x3D; list(np.unique(p.catIds))  ##唯一gt类别id，不包括背景</span><br><span class="line">        p.maxDets &#x3D; sorted(p.maxDets)</span><br><span class="line">        self.params&#x3D;p</span><br><span class="line"></span><br><span class="line">        self._prepare()</span><br><span class="line">        # loop through images, area range, max detection number</span><br><span class="line">        catIds &#x3D; p.catIds if p.useCats else [-1]</span><br><span class="line"></span><br><span class="line">        if p.iouType &#x3D;&#x3D; &#39;segm&#39; or p.iouType &#x3D;&#x3D; &#39;bbox&#39;:</span><br><span class="line">            computeIoU &#x3D; self.computeIoU</span><br><span class="line">        elif p.iouType &#x3D;&#x3D; &#39;keypoints&#39;:</span><br><span class="line">            computeIoU &#x3D; self.computeOks</span><br><span class="line"></span><br><span class="line">        ##self.ious是一个字典，每一个元素是表示一张图中某一个类别的预测框(m个)和这个类别的gt(n个)的iou矩阵(m,n)</span><br><span class="line">        self.ious &#x3D; &#123;(imgId, catId): computeIoU(imgId, catId) \</span><br><span class="line">                        for imgId in p.imgIds</span><br><span class="line">                        for catId in catIds&#125;</span><br><span class="line"></span><br><span class="line">        evaluateImg &#x3D; self.evaluateImg</span><br><span class="line">        maxDet &#x3D; p.maxDets[-1]</span><br><span class="line">        ##self.evalImgs是列表，每一个元素是字典，存储的是单张图片，一种类别，特定areaRng下的预测框和gt的匹配结果(在不同的阈值下)</span><br><span class="line">        self.evalImgs &#x3D; [evaluateImg(imgId, catId, areaRng, maxDet)</span><br><span class="line">                 for catId in catIds</span><br><span class="line">                 for areaRng in p.areaRng</span><br><span class="line">                 for imgId in p.imgIds</span><br><span class="line">             ]</span><br><span class="line">        self._paramsEval &#x3D; copy.deepcopy(self.params)</span><br><span class="line">        toc &#x3D; time.time()</span><br><span class="line">        print(&#39;DONE (t&#x3D;&#123;:0.2f&#125;s).&#39;.format(toc-tic))</span><br><span class="line"></span><br><span class="line">    def computeIoU(self, imgId, catId):</span><br><span class="line">        p &#x3D; self.params</span><br><span class="line">        if p.useCats:</span><br><span class="line">            gt &#x3D; self._gts[imgId,catId]</span><br><span class="line">            dt &#x3D; self._dts[imgId,catId]</span><br><span class="line">        else:</span><br><span class="line">            gt &#x3D; [_ for cId in p.catIds for _ in self._gts[imgId,cId]]</span><br><span class="line">            dt &#x3D; [_ for cId in p.catIds for _ in self._dts[imgId,cId]]</span><br><span class="line">        if len(gt) &#x3D;&#x3D; 0 and len(dt) &#x3D;&#x3D;0:</span><br><span class="line">            return []</span><br><span class="line">        ##inds是score从大到小排列的索引</span><br><span class="line">        inds &#x3D; np.argsort([-d[&#39;score&#39;] for d in dt], kind&#x3D;&#39;mergesort&#39;)</span><br><span class="line">        ##将此处的dt(一张图片一个类别的所有100个检测框(dt大于100个检测框的，按置信度取前100个(100个由p.maxDets设定))按置信度从大到小排列)</span><br><span class="line">        ##注意是一张图片一种类别的预测框不超过p.maxDets[-1]个，而不是一张图片的预测框不超过这么多，除非设置忽视类别，那就等价于一张图片的总的预测框不多于p.maxDets[-1]</span><br><span class="line">        dt &#x3D; [dt[i] for i in inds]  </span><br><span class="line">        if len(dt) &gt; p.maxDets[-1]:</span><br><span class="line">            dt&#x3D;dt[0:p.maxDets[-1]]</span><br><span class="line"></span><br><span class="line">        if p.iouType &#x3D;&#x3D; &#39;segm&#39;:</span><br><span class="line">            g &#x3D; [g[&#39;segmentation&#39;] for g in gt]</span><br><span class="line">            d &#x3D; [d[&#39;segmentation&#39;] for d in dt]</span><br><span class="line">        elif p.iouType &#x3D;&#x3D; &#39;bbox&#39;:</span><br><span class="line">            g &#x3D; [g[&#39;bbox&#39;] for g in gt]</span><br><span class="line">            d &#x3D; [d[&#39;bbox&#39;] for d in dt]</span><br><span class="line">        else:</span><br><span class="line">            raise Exception(&#39;unknown iouType for iou computation&#39;)</span><br><span class="line">        ##gt和dt是一张图片的一种类别的所有框信息；其中dt中只取p.maxDets[-1]个检测框，按置信度从大到小排序；</span><br><span class="line">        ##g和d是从gt和dt中获取的segmentation信息(分割任务)，检测任务取得是bbox信息；</span><br><span class="line">        # compute iou between each dt and gt region</span><br><span class="line">        iscrowd &#x3D; [int(o[&#39;iscrowd&#39;]) for o in gt]</span><br><span class="line">        ious &#x3D; maskUtils.iou(d,g,iscrowd)   ##ious是(m,n),m是d的个数，即模型的预测检测框个数，n是g的框个数</span><br><span class="line">        return ious</span><br><span class="line"></span><br><span class="line">    def computeOks(self, imgId, catId):</span><br><span class="line">        p &#x3D; self.params</span><br><span class="line">        # dimention here should be Nxm</span><br><span class="line">        gts &#x3D; self._gts[imgId, catId]</span><br><span class="line">        dts &#x3D; self._dts[imgId, catId]</span><br><span class="line">        inds &#x3D; np.argsort([-d[&#39;score&#39;] for d in dts], kind&#x3D;&#39;mergesort&#39;)</span><br><span class="line">        dts &#x3D; [dts[i] for i in inds]</span><br><span class="line">        if len(dts) &gt; p.maxDets[-1]:</span><br><span class="line">            dts &#x3D; dts[0:p.maxDets[-1]]</span><br><span class="line">        # if len(gts) &#x3D;&#x3D; 0 and len(dts) &#x3D;&#x3D; 0:</span><br><span class="line">        if len(gts) &#x3D;&#x3D; 0 or len(dts) &#x3D;&#x3D; 0:</span><br><span class="line">            return []</span><br><span class="line">        ious &#x3D; np.zeros((len(dts), len(gts)))</span><br><span class="line">        sigmas &#x3D; p.kpt_oks_sigmas</span><br><span class="line">        vars &#x3D; (sigmas * 2)**2</span><br><span class="line">        k &#x3D; len(sigmas)</span><br><span class="line">        # compute oks between each detection and ground truth object</span><br><span class="line">        for j, gt in enumerate(gts):</span><br><span class="line">            # create bounds for ignore regions(double the gt bbox)</span><br><span class="line">            g &#x3D; np.array(gt[&#39;keypoints&#39;])</span><br><span class="line">            xg &#x3D; g[0::3]; yg &#x3D; g[1::3]; vg &#x3D; g[2::3]</span><br><span class="line">            k1 &#x3D; np.count_nonzero(vg &gt; 0)</span><br><span class="line">            bb &#x3D; gt[&#39;bbox&#39;]</span><br><span class="line">            x0 &#x3D; bb[0] - bb[2]; x1 &#x3D; bb[0] + bb[2] * 2</span><br><span class="line">            y0 &#x3D; bb[1] - bb[3]; y1 &#x3D; bb[1] + bb[3] * 2</span><br><span class="line">            for i, dt in enumerate(dts):</span><br><span class="line">                d &#x3D; np.array(dt[&#39;keypoints&#39;])</span><br><span class="line">                xd &#x3D; d[0::3]; yd &#x3D; d[1::3]</span><br><span class="line">                if k1&gt;0:</span><br><span class="line">                    # measure the per-keypoint distance if keypoints visible</span><br><span class="line">                    dx &#x3D; xd - xg</span><br><span class="line">                    dy &#x3D; yd - yg</span><br><span class="line">                else:</span><br><span class="line">                    # measure minimum distance to keypoints in (x0,y0) &amp; (x1,y1)</span><br><span class="line">                    z &#x3D; np.zeros((k))</span><br><span class="line">                    dx &#x3D; np.max((z, x0-xd),axis&#x3D;0)+np.max((z, xd-x1),axis&#x3D;0)</span><br><span class="line">                    dy &#x3D; np.max((z, y0-yd),axis&#x3D;0)+np.max((z, yd-y1),axis&#x3D;0)</span><br><span class="line">                e &#x3D; (dx**2 + dy**2) &#x2F; vars &#x2F; (gt[&#39;area&#39;]+np.spacing(1)) &#x2F; 2</span><br><span class="line">                if k1 &gt; 0:</span><br><span class="line">                    e&#x3D;e[vg &gt; 0]</span><br><span class="line">                ious[i, j] &#x3D; np.sum(np.exp(-e)) &#x2F; e.shape[0]</span><br><span class="line">        return ious</span><br><span class="line"></span><br><span class="line">    def evaluateImg(self, imgId, catId, aRng, maxDet):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        perform evaluation for single category and image</span><br><span class="line">        :return: dict (single image results)</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        p &#x3D; self.params</span><br><span class="line">        if p.useCats:</span><br><span class="line">            gt &#x3D; self._gts[imgId,catId]</span><br><span class="line">            dt &#x3D; self._dts[imgId,catId]</span><br><span class="line">        else:</span><br><span class="line">            gt &#x3D; [_ for cId in p.catIds for _ in self._gts[imgId,cId]]</span><br><span class="line">            dt &#x3D; [_ for cId in p.catIds for _ in self._dts[imgId,cId]]</span><br><span class="line">        if len(gt) &#x3D;&#x3D; 0 and len(dt) &#x3D;&#x3D;0:</span><br><span class="line">            return None</span><br><span class="line"></span><br><span class="line">        for g in gt:</span><br><span class="line">            if g[&#39;ignore&#39;] or (g[&#39;area&#39;]&lt;aRng[0] or g[&#39;area&#39;]&gt;aRng[1]):</span><br><span class="line">                g[&#39;_ignore&#39;] &#x3D; 1</span><br><span class="line">            else:</span><br><span class="line">                g[&#39;_ignore&#39;] &#x3D; 0</span><br><span class="line"></span><br><span class="line">        # sort dt highest score first, sort gt ignore last</span><br><span class="line">        gtind &#x3D; np.argsort([g[&#39;_ignore&#39;] for g in gt], kind&#x3D;&#39;mergesort&#39;)</span><br><span class="line">        gt &#x3D; [gt[i] for i in gtind]</span><br><span class="line">        dtind &#x3D; np.argsort([-d[&#39;score&#39;] for d in dt], kind&#x3D;&#39;mergesort&#39;)</span><br><span class="line">        dt &#x3D; [dt[i] for i in dtind[0:maxDet]]</span><br><span class="line"></span><br><span class="line">        iscrowd &#x3D; [int(o[&#39;iscrowd&#39;]) for o in gt]</span><br><span class="line"></span><br><span class="line">        # load computed ious</span><br><span class="line">        ##两种情况，一张图片中，一种类别的gt存在，则</span><br><span class="line">        ious &#x3D; self.ious[imgId, catId][:, gtind] if len(self.ious[imgId, catId]) &gt; 0 else self.ious[imgId, catId]</span><br><span class="line"></span><br><span class="line">        T &#x3D; len(p.iouThrs)</span><br><span class="line">        G &#x3D; len(gt)</span><br><span class="line">        D &#x3D; len(dt)</span><br><span class="line">        gtm  &#x3D; np.zeros((T,G))  ##存储的是每一个iou阈值、p.maxDet[-1]下的gt能够匹配到的最大iou对应的模型预测框的id，匹配不到的值是0；</span><br><span class="line">        dtm  &#x3D; np.zeros((T,D))   ##存储的是每一个iou阈值下的模型预测框匹配到的gt的id，匹配不到的是0；</span><br><span class="line">        gtIg &#x3D; np.array([g[&#39;_ignore&#39;] for g in gt])</span><br><span class="line">        dtIg &#x3D; np.zeros((T,D))   ##表示每一个阈值下的预测框匹配到的gt是否需要ignore</span><br><span class="line"></span><br><span class="line">        ##dt已经按照置信度排过序，gt已经按照ignore排过位置，非ignore在前，ignore在后面</span><br><span class="line">        ##下面的if里面实现的功能是每一个iou阈值下，遍历预测框(预测框已经按置信度从大到小排序)，一个预测框和gt匹配上，则</span><br><span class="line">        ##另一个预测框不能再通过iou和这个gt进行匹配</span><br><span class="line">        if not len(ious)&#x3D;&#x3D;0:</span><br><span class="line">            for tind, t in enumerate(p.iouThrs):</span><br><span class="line">                for dind, d in enumerate(dt):</span><br><span class="line">                    # information about best match so far (m&#x3D;-1 -&gt; unmatched)</span><br><span class="line">                    iou &#x3D; min([t,1-1e-10])</span><br><span class="line">                    # # 如果m&#x3D; -1 代表这个dt没有得到匹配 m代表dt匹配的最好的gt的索引下标</span><br><span class="line">                    m   &#x3D; -1</span><br><span class="line">                    for gind, g in enumerate(gt):</span><br><span class="line">                        # if this gt already matched, and not a crowd, continue</span><br><span class="line">                        if gtm[tind,gind]&gt;0 and not iscrowd[gind]:</span><br><span class="line">                            continue</span><br><span class="line">                        # if dt matched to reg gt, and on ignore gt, stop</span><br><span class="line">                        if m&gt;-1 and gtIg[m]&#x3D;&#x3D;0 and gtIg[gind]&#x3D;&#x3D;1:</span><br><span class="line">                            break</span><br><span class="line">                        # continue to next gt unless better match made</span><br><span class="line">                        if ious[dind,gind] &lt; iou:</span><br><span class="line">                            continue</span><br><span class="line">                        # if match successful and best so far, store appropriately</span><br><span class="line">                        iou&#x3D;ious[dind,gind]</span><br><span class="line">                        m&#x3D;gind</span><br><span class="line">                    # if match made store id of match for both dt and gt</span><br><span class="line">                    if m &#x3D;&#x3D;-1:</span><br><span class="line">                        continue</span><br><span class="line">                    dtIg[tind,dind] &#x3D; gtIg[m]   ##对应的能匹配上gt的预测框是否ignore</span><br><span class="line">                    dtm[tind,dind]  &#x3D; gt[m][&#39;id&#39;]  ##dt匹配上的gt的id</span><br><span class="line">                    gtm[tind,m]     &#x3D; d[&#39;id&#39;]   ##gt中的框匹配上的预测框的id</span><br><span class="line">        # set unmatched detections outside of area range to ignore</span><br><span class="line">        ##将dtm中没有匹配到gt的预测框，同时预测框的area在指定的aRng范围外，则设置对应的预测框为ignore</span><br><span class="line">        a &#x3D; np.array([d[&#39;area&#39;]&lt;aRng[0] or d[&#39;area&#39;]&gt;aRng[1] for d in dt]).reshape((1, len(dt)))</span><br><span class="line">        dtIg &#x3D; np.logical_or(dtIg, np.logical_and(dtm&#x3D;&#x3D;0, np.repeat(a,T,0)))</span><br><span class="line">        # store results for given image and category</span><br><span class="line">        return &#123;</span><br><span class="line">                &#39;image_id&#39;:     imgId,</span><br><span class="line">                &#39;category_id&#39;:  catId,</span><br><span class="line">                &#39;aRng&#39;:         aRng,   ##aRng范围外的gt和未匹配到gt的预测框但在aRng范围外都是ignore，匹配到gt的预测框在aRng范围外正常计算，不ignore</span><br><span class="line">                &#39;maxDet&#39;:       maxDet,  ##这里是p.maxDets[-1]</span><br><span class="line">                &#39;dtIds&#39;:        [d[&#39;id&#39;] for d in dt], ##已经排过序的预测框id</span><br><span class="line">                &#39;gtIds&#39;:        [g[&#39;id&#39;] for g in gt],</span><br><span class="line">                &#39;dtMatches&#39;:    dtm,  ##(T,D) 其中D是已经按置信度排除的bbox</span><br><span class="line">                &#39;gtMatches&#39;:    gtm,  ##(T,G) G是按照aRng等信息排序的不ignore在前，ignore在后的gt</span><br><span class="line">                &#39;dtScores&#39;:     [d[&#39;score&#39;] for d in dt],   ##已经排过序的score</span><br><span class="line">                &#39;gtIgnore&#39;:     gtIg,  ##G指的是单张图片特定aRng的gt是否ignore信息</span><br><span class="line">                &#39;dtIgnore&#39;:     dtIg, ##(T,D)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">    def accumulate(self, p &#x3D; None):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        Accumulate per image evaluation results and store the result in self.eval</span><br><span class="line">        :param p: input params for evaluation</span><br><span class="line">        :return: None</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        print(&#39;Accumulating evaluation results...&#39;)</span><br><span class="line">        tic &#x3D; time.time()</span><br><span class="line">        if not self.evalImgs:</span><br><span class="line">            print(&#39;Please run evaluate() first&#39;)</span><br><span class="line">        # allows input customized parameters</span><br><span class="line">        if p is None:</span><br><span class="line">            p &#x3D; self.params</span><br><span class="line">        p.catIds &#x3D; p.catIds if p.useCats &#x3D;&#x3D; 1 else [-1]</span><br><span class="line">        T           &#x3D; len(p.iouThrs)  ##设置的iou阈值的个数</span><br><span class="line">        R           &#x3D; len(p.recThrs)  ##设置的召回的recThrs阈值的个数</span><br><span class="line">        K           &#x3D; len(p.catIds) if p.useCats else 1</span><br><span class="line">        A           &#x3D; len(p.areaRng)</span><br><span class="line">        M           &#x3D; len(p.maxDets)</span><br><span class="line">        G_num       &#x3D; 7800        ##设置的该评估用的数据集的gt总数,可以事先通过cvat查看标注的bbox个数，或者自己评估性的设置一个数</span><br><span class="line">        precision   &#x3D; -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories ##这个是存储不同的rec值下的p值，相当于存储了pr曲线的采样点</span><br><span class="line">        recall      &#x3D; -np.ones((T,K,A,M))</span><br><span class="line">        precision_s &#x3D; -np.ones((T,K,A,M))   ##真实的精确率值</span><br><span class="line">        scores      &#x3D; -np.ones((T,R,K,A,M))</span><br><span class="line">        DTMatch     &#x3D; -np.ones((T,K,A,G_num,M))</span><br><span class="line"></span><br><span class="line">        # create dictionary for future indexing</span><br><span class="line">        _pe &#x3D; self._paramsEval</span><br><span class="line">        catIds &#x3D; _pe.catIds if _pe.useCats else [-1]</span><br><span class="line">        setK &#x3D; set(catIds)</span><br><span class="line">        setA &#x3D; set(map(tuple, _pe.areaRng))</span><br><span class="line">        setM &#x3D; set(_pe.maxDets)</span><br><span class="line">        setI &#x3D; set(_pe.imgIds)</span><br><span class="line">        # get inds to evaluate</span><br><span class="line">        k_list &#x3D; [n for n, k in enumerate(p.catIds)  if k in setK]</span><br><span class="line">        m_list &#x3D; [m for n, m in enumerate(p.maxDets) if m in setM]</span><br><span class="line">        a_list &#x3D; [n for n, a in enumerate(map(lambda x: tuple(x), p.areaRng)) if a in setA]</span><br><span class="line">        i_list &#x3D; [n for n, i in enumerate(p.imgIds)  if i in setI]</span><br><span class="line">        I0 &#x3D; len(_pe.imgIds)</span><br><span class="line">        A0 &#x3D; len(_pe.areaRng)</span><br><span class="line"></span><br><span class="line">        ##根据self.evalImgs的存储形式，遍历时最里层是img_id、次外层是aRng、最外层是类别id</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">            self.evalImgs &#x3D; [evaluateImg(imgId, catId, areaRng, maxDet)</span><br><span class="line">                for catId in catIds</span><br><span class="line">                for areaRng in p.areaRng</span><br><span class="line">                for imgId in p.imgIds</span><br><span class="line">            ]</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        # retrieve E at each category, area range, and max number of detections</span><br><span class="line">        for k, k0 in enumerate(k_list):  ##类别的索引下标遍历</span><br><span class="line">            Nk &#x3D; k0*A0*I0   </span><br><span class="line">            for a, a0 in enumerate(a_list):  ##aRng的遍历</span><br><span class="line">                Na &#x3D; a0*I0</span><br><span class="line">                for m, maxDet in enumerate(m_list):</span><br><span class="line">                    E &#x3D; [self.evalImgs[Nk + Na + i] for i in i_list]</span><br><span class="line">                    E &#x3D; [e for e in E if not e is None]</span><br><span class="line">                    if len(E) &#x3D;&#x3D; 0:</span><br><span class="line">                        continue</span><br><span class="line">                    ##特定类别、特定aRng的所有图片中每一张图片的maxDet个预测框</span><br><span class="line">                    dtScores &#x3D; np.concatenate([e[&#39;dtScores&#39;][0:maxDet] for e in E])  </span><br><span class="line"></span><br><span class="line">                    # different sorting method generates slightly different results.</span><br><span class="line">                    # mergesort is used to be consistent as Matlab implementation.</span><br><span class="line">                    inds &#x3D; np.argsort(-dtScores, kind&#x3D;&#39;mergesort&#39;)</span><br><span class="line">                    ##是将特定类别，特定aRng的所有图片的预测框</span><br><span class="line">                    # (每张图片特定类别、aRng取置信度从大到小的maxDet个框)</span><br><span class="line">                    #的置信度拉成一位数组，然后再次从大到小排列；</span><br><span class="line">                    dtScoresSorted &#x3D; dtScores[inds]</span><br><span class="line">                    ##dtm、dtIg维度是(T,maxDet个数*图片个数)</span><br><span class="line">                    dtm  &#x3D; np.concatenate([e[&#39;dtMatches&#39;][:,0:maxDet] for e in E], axis&#x3D;1)[:,inds]</span><br><span class="line">                    dtIg &#x3D; np.concatenate([e[&#39;dtIgnore&#39;][:,0:maxDet]  for e in E], axis&#x3D;1)[:,inds]</span><br><span class="line">                    gtIg &#x3D; np.concatenate([e[&#39;gtIgnore&#39;] for e in E])  ##gtIg维度是(图片个数，G)</span><br><span class="line">                    npig &#x3D; np.count_nonzero(gtIg&#x3D;&#x3D;0 )   ##gt不ignore的个数</span><br><span class="line">                    if npig &#x3D;&#x3D; 0:</span><br><span class="line">                        continue</span><br><span class="line"></span><br><span class="line">                    ##dtm、dtIg维度是(T,maxDet个数*图片个数)</span><br><span class="line">                    tps &#x3D; np.logical_and(               dtm,  np.logical_not(dtIg) )</span><br><span class="line">                    fps &#x3D; np.logical_and(np.logical_not(dtm), np.logical_not(dtIg) )</span><br><span class="line"></span><br><span class="line">                    ###GT是特定类别、特定aRng、maxDet下所有图片的gt能被预测到的情况</span><br><span class="line">                    #GT &#x3D; [self.evalImgs[Nk + Na + i] for i in i_list]</span><br><span class="line">                    gtmatch_id &#x3D; tps * dtm  ##(T,图片个数*maxDet)</span><br><span class="line">                    indice_gt &#x3D; [np.where(i&gt; 0)for i in gtmatch_id]</span><br><span class="line">                    unique_id &#x3D; np.array([np.unique(i[indice_gt[p]]) for p,i in enumerate(gtmatch_id)])  #(T,d（不同的iou下，maxDet下的预测框能检测到的gt，所以维度d维度不一样）)</span><br><span class="line">                    #gtmatch &#x3D; np.concatenate([e[&#39;gtMatches&#39;] for e in GT], axis&#x3D;1)  ##(T,图片个数*G)</span><br><span class="line">                    #gt_total_num_k_a &#x3D; gtmatch.shape[1]</span><br><span class="line">                    #(T,K,A,G_num,M)</span><br><span class="line">                    for j,id in enumerate(unique_id):</span><br><span class="line">                        gt_total_num_k_a &#x3D; len(id)</span><br><span class="line">                        DTMatch[j,k,a,:gt_total_num_k_a, m] &#x3D; id  ##存储的是预测框能匹配上的gt的id</span><br><span class="line"></span><br><span class="line">                    tp_sum &#x3D; np.cumsum(tps, axis&#x3D;1).astype(dtype&#x3D;np.float)</span><br><span class="line">                    fp_sum &#x3D; np.cumsum(fps, axis&#x3D;1).astype(dtype&#x3D;np.float)</span><br><span class="line"></span><br><span class="line">                    for t, (tp, fp) in enumerate(zip(tp_sum, fp_sum)):</span><br><span class="line">                        tp &#x3D; np.array(tp)</span><br><span class="line">                        fp &#x3D; np.array(fp)</span><br><span class="line">                        nd &#x3D; len(tp)</span><br><span class="line">                        rc &#x3D; tp &#x2F; npig</span><br><span class="line">                        pr &#x3D; tp &#x2F; (fp+tp+np.spacing(1))</span><br><span class="line">                        q  &#x3D; np.zeros((R,))   ##特定召回率下的precision值(pr曲线)</span><br><span class="line">                        ss &#x3D; np.zeros((R,))   ##特定召回率下的对应的bbox的置信度</span><br><span class="line"></span><br><span class="line">                        if nd:</span><br><span class="line">                            recall[t,k,a,m] &#x3D; rc[-1]</span><br><span class="line">                            precision_s[t,k,a,m] &#x3D; pr[-1]</span><br><span class="line">                        else:</span><br><span class="line">                            recall[t,k,a,m] &#x3D; 0</span><br><span class="line">                            precision_s[t,k,a,m] &#x3D; 0</span><br><span class="line"></span><br><span class="line">                        # numpy is slow without cython optimization for accessing elements</span><br><span class="line">                        # use python array gets significant speed improvement</span><br><span class="line">                        pr &#x3D; pr.tolist(); q &#x3D; q.tolist()</span><br><span class="line"></span><br><span class="line">                        for i in range(nd-1, 0, -1):</span><br><span class="line">                            if pr[i] &gt; pr[i-1]:</span><br><span class="line">                                pr[i-1] &#x3D; pr[i]</span><br><span class="line"></span><br><span class="line">                        ##这里调用的np.searchsorted表示p.recThrs中每一个值能插入到rc中的位置索引，其中rc必须是升序</span><br><span class="line">                        inds &#x3D; np.searchsorted(rc, p.recThrs, side&#x3D;&#39;left&#39;)</span><br><span class="line">                        try:</span><br><span class="line">                            for ri, pi in enumerate(inds):</span><br><span class="line">                                q[ri] &#x3D; pr[pi]</span><br><span class="line">                                ss[ri] &#x3D; dtScoresSorted[pi]</span><br><span class="line">                        except:</span><br><span class="line">                            pass</span><br><span class="line">                        precision[t,:,k,a,m] &#x3D; np.array(q)</span><br><span class="line">                        scores[t,:,k,a,m] &#x3D; np.array(ss)</span><br><span class="line">                        </span><br><span class="line">        self.eval &#x3D; &#123;</span><br><span class="line">            &#39;params&#39;: p,</span><br><span class="line">            &#39;counts&#39;: [T, R, K, A, M],</span><br><span class="line">            &#39;date&#39;: datetime.datetime.now().strftime(&#39;%Y-%m-%d %H:%M:%S&#39;),</span><br><span class="line">            &#39;precision&#39;: precision,  ##(T,R,K,A,M)</span><br><span class="line">            &#39;recall&#39;:   recall,    ##(T,K,A,M)</span><br><span class="line">            &#39;precision_s&#39;: precision_s,</span><br><span class="line">            &#39;scores&#39;: scores,  ##(T,R,K,A,M)</span><br><span class="line">            &#39;DTMatch&#39;: DTMatch,</span><br><span class="line">        &#125;</span><br><span class="line">        toc &#x3D; time.time()</span><br><span class="line">        print(&#39;DONE (t&#x3D;&#123;:0.2f&#125;s).&#39;.format( toc-tic))</span><br><span class="line"></span><br><span class="line">    def get_good_predict_data(self):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        该函数主要用于得到评估数据集中gt成功预测的图片，相反可以得到gt预测不好的图片用于离线困难数据挑选；</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        def get_imgid_excellent_predict(save_path, iouThr, areaRng, maxDets, catId):</span><br><span class="line">            </span><br><span class="line">            p &#x3D; self.params</span><br><span class="line">            ##(T,K,A,G_num,M)</span><br><span class="line">            DTMatch &#x3D; self.eval[&#39;DTMatch&#39;]</span><br><span class="line">            aind &#x3D; [i for i, aRng in enumerate(p.areaRngLbl) if aRng &#x3D;&#x3D; areaRng]</span><br><span class="line">            mind &#x3D; [i for i, mDet in enumerate(p.maxDets) if mDet &#x3D;&#x3D; maxDets] </span><br><span class="line">            cind &#x3D;  [i for i, cat in enumerate(p.catIds) if cat in catId]      </span><br><span class="line">            if iouThr is not None:</span><br><span class="line">                t &#x3D; np.where(iouThr &#x3D;&#x3D; p.iouThrs)[0]</span><br><span class="line">                DTMatch &#x3D; DTMatch[t]</span><br><span class="line">            ##应该分类别计算，不然统一取unique的话，一张图中只要有一种类别的一个gt被检测出来，这张图片后续就会认为是预测较好</span><br><span class="line">            ##的样本，但是该图片中同一种类的其他gt或者其他类的gt可能完全没检出，因此需要分类别计算</span><br><span class="line">            ##iou阈值混在一起没问题，或者取特定的iou阈值就可以了</span><br><span class="line">            ##其实也就是将gt的id和这里的np.unique(DTMatch[:,cind,aind,:,mind])取差集就知道漏检情况了，因为id是指的gt的框的索引</span><br><span class="line">            gt_imgid_cat_id &#x3D; copy.deepcopy(self.gt_imgid_cat_id)</span><br><span class="line">            DTMatch &#x3D; DTMatch[:,cind,aind,:,mind]</span><br><span class="line">            #print(DTMatch.shape)</span><br><span class="line">            for i,cat in enumerate(catId):</span><br><span class="line">                DTMatch_catid &#x3D; np.unique(DTMatch[:,i,:])</span><br><span class="line">                DTMatch_catid &#x3D; np.delete(DTMatch_catid,0)</span><br><span class="line">                for j in DTMatch_catid:</span><br><span class="line">                    image_id &#x3D; self.gt_id2img_id[j]</span><br><span class="line">                    gt_imgid_cat_id[image_id][cat].remove(j)</span><br><span class="line">            </span><br><span class="line">            </span><br><span class="line">            null_leak_det &#x3D; []   ##存储完全检测出gt bbox的图片id</span><br><span class="line">            for image_id_i in gt_imgid_cat_id.keys():</span><br><span class="line">                num_empty &#x3D; 0</span><br><span class="line">                for catId_i in catId:</span><br><span class="line">                #for catId_i in gt_imgid_cat_id[image_id_i]:</span><br><span class="line">                    if len(gt_imgid_cat_id[image_id_i][catId_i]) &#x3D;&#x3D;0:</span><br><span class="line">                        num_empty +&#x3D; 1</span><br><span class="line">                        #RuntimeError: dictionary changed size during iteration</span><br><span class="line">                        #gt_imgid_cat_id[image_id_i].pop(catId_i)</span><br><span class="line">                #if num_empty &#x3D;&#x3D; len(self.params.catIds):</span><br><span class="line">                if num_empty &#x3D;&#x3D; len(catId):</span><br><span class="line">                    null_leak_det.append(image_id_i)</span><br><span class="line"></span><br><span class="line">            det_gt &#x3D; np.array([ self.cocoGt.loadImgs(ids&#x3D;[i])[0][&#39;file_name&#39;] for i in null_leak_det])</span><br><span class="line">            det_gt &#x3D; np.unique(det_gt)</span><br><span class="line">            np.savetxt(save_path, det_gt, fmt&#x3D;&#39;%s&#39;, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line"></span><br><span class="line">            #json_str &#x3D; json.dumps(gt_imgid_cat_id)</span><br><span class="line">            for img_id_i in null_leak_det:</span><br><span class="line">                gt_imgid_cat_id.pop(img_id_i)</span><br><span class="line"></span><br><span class="line">            #print(&#39;length of gt_imgid_cat_id&#x3D;&#123;&#125;, none_leak_det&#x3D;&#123;&#125;, imgIds&#x3D;&#123;&#125;&#39;.format(len(gt_imgid_cat_id.keys()),len(null_leak_det),len(self.params.imgIds)))</span><br><span class="line">            json_str &#x3D; repr(gt_imgid_cat_id)</span><br><span class="line">            with open(save_path.replace(&#39;.txt&#39;,&#39;.json&#39;).replace(&#39;good&#39;,&#39;leak_det&#39;), &#39;w&#39;) as json_file:</span><br><span class="line">                json_file.write(json_str)</span><br><span class="line">            </span><br><span class="line">            leak_det_gt &#x3D; gt_imgid_cat_id.keys()</span><br><span class="line">            leak_det_gt &#x3D; np.array([ self.cocoGt.loadImgs(ids&#x3D;[i])[0][&#39;file_name&#39;] for i in leak_det_gt])</span><br><span class="line">            leak_det_gt &#x3D; np.unique(leak_det_gt)</span><br><span class="line">            np.savetxt(save_path.replace(&#39;good&#39;,&#39;leak_det&#39;), leak_det_gt, fmt&#x3D;&#39;%s&#39;, delimiter&#x3D;&#39;,&#39;)</span><br><span class="line">            # DTMatch &#x3D; np.unique(DTMatch[:,cind,aind,:,mind])</span><br><span class="line">            # DTMatch &#x3D; np.delete(DTMatch,0)</span><br><span class="line">            # ###检测框对应的gt id和真实的gt的id的差集就是未检测出的gt的框的id</span><br><span class="line">            # #DTMatch.tolist().remove(-1.0)</span><br><span class="line">            # #print(DTMatch)</span><br><span class="line">            # det_gt &#x3D; np.array([ self.cocoGt.loadImgs(ids&#x3D;[self.gt_id2img_id[i]])[0][&#39;file_name&#39;] for i in DTMatch])</span><br><span class="line">            # det_gt &#x3D; np.unique(det_gt)</span><br><span class="line"></span><br><span class="line">            # # with open(save_path,&#39;w+&#39;) as file_object:</span><br><span class="line">            # #     json.dump(DTMatch,file_object)    </span><br><span class="line">            # np.savetxt(save_path, det_gt, fmt&#x3D;&#39;%s&#39;, delimiter&#x3D;&#39;,&#39;)           </span><br><span class="line">            # # f &#x3D; open(save_path,&#39;w+&#39;)</span><br><span class="line">            # # for i in DTMatch:</span><br><span class="line">            # #     f.write(i)</span><br><span class="line">            # #     f.write(&#39;\n&#39;)</span><br><span class="line">            # # f.close()</span><br><span class="line">            #print(&#39;save iou&#x3D;&#123;&#125;| areaRng&#x3D;&#123;&#125;| maxDets&#x3D;&#123;&#125;| catId&#x3D;&#123;&#125; to &#123;&#125;&#39;.format(iouThr, areaRng, maxDets, catId, save_path))</span><br><span class="line"></span><br><span class="line">        save_path &#x3D; r&#39;.&#x2F;good_predict.txt&#39;</span><br><span class="line">        get_imgid_excellent_predict(save_path, iouThr&#x3D;.5, areaRng&#x3D;&#39;all&#39;, maxDets&#x3D;self.params.maxDets[0], catId&#x3D;[self.params.catIds[2]])</span><br><span class="line"></span><br><span class="line">    def summarize(self):</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        Compute and display summary metrics for evaluation results.</span><br><span class="line">        Note this functin can *only* be applied on the default parameter setting</span><br><span class="line">        &#39;&#39;&#39;</span><br><span class="line">        def _summarize( ap&#x3D;1, iouThr&#x3D;None, areaRng&#x3D;&#39;all&#39;, maxDets&#x3D;100 , catId&#x3D;self.params.catIds):</span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            precision   &#x3D; -np.ones((T,R,K,A,M)) # -1 for the precision of absent categories ##这个是存储不同的rec值下的p值，相当于存储了pr曲线的采样点</span><br><span class="line">            recall      &#x3D; -np.ones((T,K,A,M))</span><br><span class="line">            precision_s &#x3D; -np.ones((T,K,A,M))   ##真实的精确率值</span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            p &#x3D; self.params</span><br><span class="line">            iStr &#x3D; &#39; &#123;:&lt;18&#125; &#123;&#125; @[ IoU&#x3D;&#123;:&lt;9&#125; | area&#x3D;&#123;:&gt;6s&#125; | maxDets&#x3D;&#123;:&gt;3d&#125; ] &#x3D; &#123;:0.3f&#125;&#39;</span><br><span class="line">            # titleStr &#x3D; &#39;Average Precision&#39; if ap &#x3D;&#x3D; 1 else &#39;Average Recall&#39;</span><br><span class="line">            # typeStr &#x3D; &#39;(AP)&#39; if ap&#x3D;&#x3D;1 else &#39;(AR)&#39;</span><br><span class="line">            #titleStr &#x3D; &#39;Average Precision&#39; if ap &#x3D;&#x3D; 1 else &#39;Average Recall&#39;</span><br><span class="line">            #typeStr &#x3D; &#39;(AP)&#39; if ap&#x3D;&#x3D;1 else &#39;(AR)&#39;</span><br><span class="line">            iouStr &#x3D; &#39;&#123;:0.2f&#125;:&#123;:0.2f&#125;&#39;.format(p.iouThrs[0], p.iouThrs[-1]) \</span><br><span class="line">                if iouThr is None else &#39;&#123;:0.2f&#125;&#39;.format(iouThr)</span><br><span class="line"></span><br><span class="line">            aind &#x3D; [i for i, aRng in enumerate(p.areaRngLbl) if aRng &#x3D;&#x3D; areaRng]</span><br><span class="line">            mind &#x3D; [i for i, mDet in enumerate(p.maxDets) if mDet &#x3D;&#x3D; maxDets]</span><br><span class="line"></span><br><span class="line">            cind &#x3D;  [i for i, cat in enumerate(p.catIds) if cat in catId]</span><br><span class="line"></span><br><span class="line">            if ap &#x3D;&#x3D; 1:</span><br><span class="line">                titleStr &#x3D; &#39;Average P-R curve Area&#39;</span><br><span class="line">                typeStr &#x3D; &#39;(mAP)&#39;</span><br><span class="line">                # dimension of precision: [TxRxKxAxM]</span><br><span class="line">                s &#x3D; self.eval[&#39;precision&#39;]</span><br><span class="line">                # IoU</span><br><span class="line">                if iouThr is not None:</span><br><span class="line">                    t &#x3D; np.where(iouThr &#x3D;&#x3D; p.iouThrs)[0]</span><br><span class="line">                    s &#x3D; s[t]</span><br><span class="line">                #s &#x3D; s[:,:,:,aind,mind]</span><br><span class="line">                s &#x3D; s[:,:,cind,aind,mind]</span><br><span class="line">            elif ap &#x3D;&#x3D; 0:</span><br><span class="line">                titleStr &#x3D; &#39;Average Recall&#39;</span><br><span class="line">                typeStr &#x3D; &#39;(AR)&#39;</span><br><span class="line">                # dimension of recall: [TxKxAxM]</span><br><span class="line">                s &#x3D; self.eval[&#39;recall&#39;]</span><br><span class="line">                if iouThr is not None:</span><br><span class="line">                    t &#x3D; np.where(iouThr &#x3D;&#x3D; p.iouThrs)[0]</span><br><span class="line">                    s &#x3D; s[t]</span><br><span class="line">                #s &#x3D; s[:,:,aind,mind]</span><br><span class="line">                s &#x3D; s[:,cind,aind,mind]</span><br><span class="line"></span><br><span class="line">            else:</span><br><span class="line">                titleStr &#x3D; &#39;Average Precision&#39;</span><br><span class="line">                typeStr &#x3D; &#39;(AP)&#39;</span><br><span class="line">                # dimension of precision: [TxKxAxM]</span><br><span class="line">                s &#x3D; self.eval[&#39;precision_s&#39;]</span><br><span class="line">                # IoU</span><br><span class="line">                if iouThr is not None:</span><br><span class="line">                    t &#x3D; np.where(iouThr &#x3D;&#x3D; p.iouThrs)[0]</span><br><span class="line">                    s &#x3D; s[t]</span><br><span class="line">                #s &#x3D; s[:,:,aind,mind]</span><br><span class="line">                s &#x3D; s[:,cind,aind,mind]</span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            if len(s[s&gt;-1])&#x3D;&#x3D;0:</span><br><span class="line">                mean_s &#x3D; -1</span><br><span class="line">            else:</span><br><span class="line">                mean_s &#x3D; np.mean(s[s&gt;-1])</span><br><span class="line">            print(iStr.format(titleStr, typeStr, iouStr, areaRng, maxDets, mean_s))</span><br><span class="line">            return mean_s</span><br><span class="line"></span><br><span class="line">        def _summarizeDets():</span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.902</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.985</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.975</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.902</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.687</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.932</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.932</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.932</span><br><span class="line"></span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line"></span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.902</span><br><span class="line">            Average P-R curve Area (mAP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.985</span><br><span class="line">            Average P-R curve Area (mAP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.975</span><br><span class="line">            Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.902</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.687</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.932</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.932</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; -1.000</span><br><span class="line">            Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.932</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.936</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.127</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.013</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.988</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.136</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.981</span><br><span class="line">            Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.135</span><br><span class="line"></span><br><span class="line">            &#39;&#39;&#39;</span><br><span class="line">            #stats &#x3D; np.zeros((12,))</span><br><span class="line">            stats &#x3D; np.zeros((31,))</span><br><span class="line">            stats[0] &#x3D; _summarize(1)</span><br><span class="line">            stats[1] &#x3D; _summarize(1, iouThr&#x3D;.5, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[2] &#x3D; _summarize(1, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[3] &#x3D; _summarize(1, areaRng&#x3D;&#39;small&#39;, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[4] &#x3D; _summarize(1, areaRng&#x3D;&#39;medium&#39;, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[5] &#x3D; _summarize(1, areaRng&#x3D;&#39;large&#39;, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[6] &#x3D; _summarize(0, iouThr&#x3D;.5, maxDets&#x3D;self.params.maxDets[1])</span><br><span class="line">            stats[7] &#x3D; _summarize(0, maxDets&#x3D;self.params.maxDets[1])</span><br><span class="line">            stats[8] &#x3D; _summarize(0, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            #stats[8] &#x3D; _summarize(0, iouThr&#x3D;.5, maxDets&#x3D;self.params.maxDets[0])</span><br><span class="line">            stats[9] &#x3D; _summarize(0, areaRng&#x3D;&#39;small&#39;, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[10] &#x3D; _summarize(0, areaRng&#x3D;&#39;medium&#39;, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[11] &#x3D; _summarize(0, areaRng&#x3D;&#39;large&#39;, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line"></span><br><span class="line">            stats[12] &#x3D; _summarize(2, maxDets&#x3D;self.params.maxDets[0])</span><br><span class="line">            stats[13] &#x3D; _summarize(2, maxDets&#x3D;self.params.maxDets[1])</span><br><span class="line">            stats[14] &#x3D; _summarize(2, maxDets&#x3D;self.params.maxDets[2])</span><br><span class="line">            stats[15] &#x3D; _summarize(2, iouThr&#x3D;.5, maxDets&#x3D;self.params.maxDets[0])</span><br><span class="line">            stats[16] &#x3D; _summarize(2, iouThr&#x3D;.5, maxDets&#x3D;self.params.maxDets[1])</span><br><span class="line">            stats[17] &#x3D; _summarize(2, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[0])</span><br><span class="line">            stats[18] &#x3D; _summarize(2, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            stats[19] &#x3D; _summarize(2, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[0]])</span><br><span class="line">            stats[20] &#x3D; _summarize(2, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[1]])   </span><br><span class="line">            stats[21] &#x3D; _summarize(2, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[2]])  </span><br><span class="line"></span><br><span class="line">            stats[22] &#x3D; _summarize(0, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[0]])</span><br><span class="line">            stats[23] &#x3D; _summarize(0, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[1]])   </span><br><span class="line">            stats[24] &#x3D; _summarize(0, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[2]]) </span><br><span class="line"></span><br><span class="line">            stats[25] &#x3D; _summarize(1, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[0]])</span><br><span class="line">            stats[26] &#x3D; _summarize(1, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[1]])   </span><br><span class="line">            stats[27] &#x3D; _summarize(1, iouThr&#x3D;.75, maxDets&#x3D;self.params.maxDets[1], catId&#x3D;[self.params.catIds[2]]) </span><br><span class="line"></span><br><span class="line">            stats[28] &#x3D; _summarize(1, areaRng&#x3D;&#39;small&#39;, maxDets&#x3D;self.params.maxDets[2], catId&#x3D;[self.params.catIds[2]])</span><br><span class="line">            stats[29] &#x3D; _summarize(1, areaRng&#x3D;&#39;medium&#39;, maxDets&#x3D;self.params.maxDets[2], catId&#x3D;[self.params.catIds[2]])   </span><br><span class="line">            stats[30] &#x3D; _summarize(1, areaRng&#x3D;&#39;large&#39;, maxDets&#x3D;self.params.maxDets[2], catId&#x3D;[self.params.catIds[2]])  </span><br><span class="line"></span><br><span class="line">            # stats &#x3D; np.zeros((2,))</span><br><span class="line">            # stats[0] &#x3D; _summarize(0, iouThr&#x3D;.8, maxDets&#x3D;self.params.maxDets[0])</span><br><span class="line">            # stats[1] &#x3D; _summarize(2, iouThr&#x3D;.8, maxDets&#x3D;self.params.maxDets[0])</span><br><span class="line">            return stats</span><br><span class="line"></span><br><span class="line">        def _summarizeKps():</span><br><span class="line">            stats &#x3D; np.zeros((10,))</span><br><span class="line">            stats[0] &#x3D; _summarize(1, maxDets&#x3D;20)</span><br><span class="line">            stats[1] &#x3D; _summarize(1, maxDets&#x3D;20, iouThr&#x3D;.5)</span><br><span class="line">            stats[2] &#x3D; _summarize(1, maxDets&#x3D;20, iouThr&#x3D;.75)</span><br><span class="line">            stats[3] &#x3D; _summarize(1, maxDets&#x3D;20, areaRng&#x3D;&#39;medium&#39;)</span><br><span class="line">            stats[4] &#x3D; _summarize(1, maxDets&#x3D;20, areaRng&#x3D;&#39;large&#39;)</span><br><span class="line">            stats[5] &#x3D; _summarize(0, maxDets&#x3D;20)</span><br><span class="line">            stats[6] &#x3D; _summarize(0, maxDets&#x3D;20, iouThr&#x3D;.5)</span><br><span class="line">            stats[7] &#x3D; _summarize(0, maxDets&#x3D;20, iouThr&#x3D;.75)</span><br><span class="line">            stats[8] &#x3D; _summarize(0, maxDets&#x3D;20, areaRng&#x3D;&#39;medium&#39;)</span><br><span class="line">            stats[9] &#x3D; _summarize(0, maxDets&#x3D;20, areaRng&#x3D;&#39;large&#39;)</span><br><span class="line">            return stats</span><br><span class="line">        if not self.eval:</span><br><span class="line">            raise Exception(&#39;Please run accumulate() first&#39;)</span><br><span class="line">        iouType &#x3D; self.params.iouType</span><br><span class="line">        if iouType &#x3D;&#x3D; &#39;segm&#39; or iouType &#x3D;&#x3D; &#39;bbox&#39;:</span><br><span class="line">            summarize &#x3D; _summarizeDets</span><br><span class="line">        elif iouType &#x3D;&#x3D; &#39;keypoints&#39;:</span><br><span class="line">            summarize &#x3D; _summarizeKps</span><br><span class="line">        self.stats &#x3D; summarize()</span><br><span class="line"></span><br><span class="line">    def __str__(self):</span><br><span class="line">        self.summarize()</span><br><span class="line"></span><br><span class="line">class Params:</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    Params for coco evaluation api</span><br><span class="line">    &#39;&#39;&#39;</span><br><span class="line">    def setDetParams(self):</span><br><span class="line">        self.imgIds &#x3D; []</span><br><span class="line">        self.catIds &#x3D; []</span><br><span class="line">        # np.arange causes trouble.  the data point on arange is slightly larger than the true value</span><br><span class="line">        #array([0.5 , 0.55, 0.6 , 0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95])</span><br><span class="line">        self.iouThrs &#x3D; np.linspace(.5, 0.95, int(np.round((0.95 - .5) &#x2F; .05)) + 1, endpoint&#x3D;True)</span><br><span class="line">        self.recThrs &#x3D; np.linspace(.0, 1.00, int(np.round((1.00 - .0) &#x2F; .01)) + 1, endpoint&#x3D;True)</span><br><span class="line">        self.maxDets &#x3D; [1, 10, 100]</span><br><span class="line">        self.areaRng &#x3D; [[0 ** 2, 1e5 ** 2], [0 ** 2, 32 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]</span><br><span class="line">        self.areaRngLbl &#x3D; [&#39;all&#39;, &#39;small&#39;, &#39;medium&#39;, &#39;large&#39;]</span><br><span class="line">        self.useCats &#x3D; 1</span><br><span class="line"></span><br><span class="line">    def setKpParams(self):</span><br><span class="line">        self.imgIds &#x3D; []</span><br><span class="line">        self.catIds &#x3D; []</span><br><span class="line">        # np.arange causes trouble.  the data point on arange is slightly larger than the true value</span><br><span class="line">        self.iouThrs &#x3D; np.linspace(.5, 0.95, int(np.round((0.95 - .5) &#x2F; .05)) + 1, endpoint&#x3D;True)</span><br><span class="line">        self.recThrs &#x3D; np.linspace(.0, 1.00, int(np.round((1.00 - .0) &#x2F; .01)) + 1, endpoint&#x3D;True)</span><br><span class="line">        self.maxDets &#x3D; [20]</span><br><span class="line">        self.areaRng &#x3D; [[0 ** 2, 1e5 ** 2], [32 ** 2, 96 ** 2], [96 ** 2, 1e5 ** 2]]</span><br><span class="line">        self.areaRngLbl &#x3D; [&#39;all&#39;, &#39;medium&#39;, &#39;large&#39;]</span><br><span class="line">        self.useCats &#x3D; 1</span><br><span class="line">        self.kpt_oks_sigmas &#x3D; np.array([.26, .25, .25, .35, .35, .79, .79, .72, .72, .62,.62, 1.07, 1.07, .87, .87, .89, .89])&#x2F;10.0</span><br><span class="line"></span><br><span class="line">    def __init__(self, iouType&#x3D;&#39;segm&#39;):</span><br><span class="line">        if iouType &#x3D;&#x3D; &#39;segm&#39; or iouType &#x3D;&#x3D; &#39;bbox&#39;:</span><br><span class="line">            self.setDetParams()</span><br><span class="line">        elif iouType &#x3D;&#x3D; &#39;keypoints&#39;:</span><br><span class="line">            self.setKpParams()</span><br><span class="line">        else:</span><br><span class="line">            raise Exception(&#39;iouType not supported&#39;)</span><br><span class="line">        self.iouType &#x3D; iouType</span><br><span class="line">        # useSegm is deprecated</span><br><span class="line">        self.useSegm &#x3D; None</span><br></pre></td></tr></table></figure><h1 id="新增功能"><a href="#新增功能" class="headerlink" title="新增功能"></a>新增功能</h1><p>通过该程序，只需要将任意检测模型的预测输出组织成result_test.json形式，ground truth保存成instances_test.json形式，然后就可以直接调用eval_coco.py进行评估。<br><strong>增加的功能：1、平均精确率的计算；2、可以指定特定CatID进行指标计算；3、保存指定条件下的检测好的样本和错检、漏检样本的名称;</strong><br><strong>具体说明：除了cocoapi本身的AP(cocoapi原始程序的AP其实是mAP，而且只能计算所有类的mAP，没有计算指定类别的mAP功能)和AR计算，对官方cocoapi修改后新增真正的AP(Average precision)计算值(新增功能1)，即修改后的cocoapi输出三种指标，AP平均精确率、AR平均召回率，mAP：pr曲线围成的面积；同时可以输出指定类别CatID(新增功能2)、指定aRng(small、medium、large)、指定maxDets(每张图每个类别的最多检测框个数)、指定IOUthr下的三个指标值；</strong><br><strong>除此之外，为了根据模型预测结果分析得到针对性的模型优化方向，可以根据指定条件（CatID、aRng、maxDets、IOUthr）计算测试集中哪些样本按照指定条件完全检出，并将这些样本名称保存在good_predict.txt文件中，同时将存在漏检或错检的bbox的样本名称和检测结果分别保存在leak_det_predict.txt和leak_det_predict.json中，这样就便于进一步分析模型在哪些测试集样本上表现不佳以及表现不佳的原因，进而可以使用离线数据增强或者其他技术对模型进行针对性优化(新增功能3)！！！</strong><br>三种类别的新版cocoapi调用结果示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">loading annotations into memory...</span><br><span class="line">Done (t&#x3D;0.02s)</span><br><span class="line">creating index...</span><br><span class="line">index created!</span><br><span class="line">Loading and preparing results...</span><br><span class="line">DONE (t&#x3D;0.03s)</span><br><span class="line">creating index...</span><br><span class="line">index created!</span><br><span class="line">-----</span><br><span class="line">length of self.params.imgIds: 30    ##总共30个样本</span><br><span class="line">self.params.catIds: [1, 2, 3]       ##总共三种类别</span><br><span class="line">Running per image evaluation...</span><br><span class="line">Evaluate annotation type *bbox*</span><br><span class="line">DONE (t&#x3D;9.42s).</span><br><span class="line">Accumulating evaluation results...</span><br><span class="line">DONE (t&#x3D;0.08s).</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.508</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.705</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.601</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; 0.298</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; 0.426</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.508</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.564</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.456</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.557</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; 0.345</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; 0.491</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.542</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.421</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.213</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;   all | maxDets&#x3D;100 ] &#x3D; 0.153</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.566</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.50      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.305</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D;  1 ] &#x3D; 0.487</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.254</span><br><span class="line"></span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.131  ##指定IOUthr</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.114</span><br><span class="line"> Average Precision  (AP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.517</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 1.000</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.579</span><br><span class="line"> Average Recall     (AR) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.066</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.951   ##不同类别catID</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.543</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.75      | area&#x3D;   all | maxDets&#x3D; 10 ] &#x3D; 0.058</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; small | maxDets&#x3D;100 ] &#x3D; 0.298   ##不同aRng</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D;medium | maxDets&#x3D;100 ] &#x3D; 0.426</span><br><span class="line"> Average P-R curve Area (mAP) @[ IoU&#x3D;0.50:0.95 | area&#x3D; large | maxDets&#x3D;100 ] &#x3D; 0.303</span><br></pre></td></tr></table></figure><p>说明：官方的cocoapi计算的mAP是IOU从0.5到0.95，每隔0.05下计算的所有类别的AP的平均值；<br>具体计算是将模型的预测框先按照每一张图片，每一种类别，按照置信度从大到小得到maxDet个框，然后将测试集中特定类别的所有框按置信度总的排序，继而再对排序后的指定类别的所有框计算tp、fp，然后对所有类别求平均得到mAP(pr曲线的面积)；因此也可以知道pr曲线是成反比例，而且是置信度递减的，置信度越低，recall越高，precision越低。<strong>注意maxDet是指一张图片，一种类别容许的最多模型检测框个数，而不是一张图片，所有类别；如果一张图片单类别检测框不足maxDet，也直接去单类别所有检测框，并不会其他补0等操作；cocoapi中​矩阵默认-1初始化，因此如果模型检测结果不理想或者测试集中没有满足条件的数据(比如数据集中只有large的物体，计算small物体的评价指标)都可能出现-1的计算结果​。</strong></p><p>上述程序已经在CenterNet工程的模型结果中正确实验过，CenterNet工程在<a href="https://github.com/xingyizhou/CenterNet/blob/master/src/test.py">test.py</a>时，会在指定保存路径下保存results.json(也就是最上面声明的模型输出<a href="https://github.com/yangsuhui/format_transfer_detection/blob/main/coco.py">保存格式</a>)，然后用这个模型检测结果和验证集的gt就可以调用了(只需要将pycocotools安装下的cocoeval.py替换成我们的修改后的文件)。</p><h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="https://github.com/cocodataset/cocoapi">COCOAPI</a><br><a href="https://github.com/dbolya/tide">TIDE</a>: A General Toolbox for Identifying Object Detection Errors(ECCV20)<br><a href="https://github.com/rafaelpadilla/Object-Detection-Metrics">Object-Detection-Metrics</a>: A Survey on Performance Metrics for Object-Detection Algorithms(IWSSIP20)</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要解析目标检测中常用的COCOAPI工具计算mAP的过程，以及增加相关功能用于更好的提供模型优化的方向。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yangsuhui.github.io/categories/Deep-Learning/"/>
    
    
    <category term="Assessment Criteria" scheme="https://yangsuhui.github.io/tags/Assessment-Criteria/"/>
    
    <category term="Object Detection" scheme="https://yangsuhui.github.io/tags/Object-Detection/"/>
    
  </entry>
  
</feed>
