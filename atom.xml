<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>AI Ants</title>
  
  <subtitle>迈向通用人工智能之路</subtitle>
  <link href="https://yangsuhui.github.io/atom.xml" rel="self"/>
  
  <link href="https://yangsuhui.github.io/"/>
  <updated>2020-10-25T13:15:45.859Z</updated>
  <id>https://yangsuhui.github.io/</id>
  
  <author>
    <name>yangsuhui</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OCR数据生成之SynthText场景文本</title>
    <link href="https://yangsuhui.github.io/p/5c8c.html"/>
    <id>https://yangsuhui.github.io/p/5c8c.html</id>
    <published>2020-10-25T06:04:55.000Z</published>
    <updated>2020-10-25T13:15:45.859Z</updated>
    
    
    <summary type="html">&lt;p&gt;本文主要介绍如何利用SynthText生成自己的场景文本。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yangsuhui.github.io/categories/Deep-Learning/"/>
    
    
    <category term="OCR" scheme="https://yangsuhui.github.io/tags/OCR/"/>
    
    <category term="Data Generation" scheme="https://yangsuhui.github.io/tags/Data-Generation/"/>
    
  </entry>
  
  <entry>
    <title>NLP之词向量</title>
    <link href="https://yangsuhui.github.io/p/ca8e.html"/>
    <id>https://yangsuhui.github.io/p/ca8e.html</id>
    <published>2020-10-01T06:22:57.000Z</published>
    <updated>2020-10-25T06:15:58.638Z</updated>
    
    
    <summary type="html">&lt;p&gt;本文主要讲述NLP中最基础的词向量的相关内容。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yangsuhui.github.io/categories/Deep-Learning/"/>
    
    
    <category term="NLP" scheme="https://yangsuhui.github.io/tags/NLP/"/>
    
    <category term="Word2Vec" scheme="https://yangsuhui.github.io/tags/Word2Vec/"/>
    
  </entry>
  
  <entry>
    <title>深度强化学习DRL从入门到放弃</title>
    <link href="https://yangsuhui.github.io/p/32f0.html"/>
    <id>https://yangsuhui.github.io/p/32f0.html</id>
    <published>2020-09-30T07:53:29.000Z</published>
    <updated>2020-10-01T06:41:19.050Z</updated>
    
    <content type="html"><![CDATA[<p><strong><code>Emphasis</code></strong>: 以下内容全是通过个人理解整理所得，如有错误，欢迎指出。<br>**<em>版权所有：杨苏辉**</em></p><h1 id="第一章"><a href="#第一章" class="headerlink" title="第一章"></a>第一章</h1><p>本章主要从Reinforcement learning(RL)基本概念出发，介绍传统强化学习的一些经典算法并辅以公式推导和程序实现。</p><p><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/">预习</a></p><h2 id="RL基本概念"><a href="#RL基本概念" class="headerlink" title="RL基本概念"></a>RL基本概念</h2><p>在经典强化学习中，智能体要和环境完成一系列交互:<br></p><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930091708.png width="300" height='300' alt='RL基本模型'/></div>（1）在每一个时刻，系统都将处于一种状态   （2）智能体将设法得到环境当前状态的观察值    （3）智能体根据观察值，结合自己历史的行为准则（策略，Policy）做出行动<br>（4）这个行动会影响环境的状态，是环境发生一定的改变。Agent将从改变后的环境中得到两部分信息：新的环境观测值和行为给出的回报。Agent可以根据新的观测值做出新的行动。<br><h3 id="DRL-vs-DL"><a href="#DRL-vs-DL" class="headerlink" title="DRL vs DL"></a>DRL vs DL</h3><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195704.png" alt="image-20191125152833795"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195705.png" alt="image-20191125153004826"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195706.png" alt="image-20191125153318152"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195707.png" alt="image-20191125153351057"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195708.png" alt="image-20191125153424249"></p><h3 id="DRL项目搭建流程-PIPELINE"><a href="#DRL项目搭建流程-PIPELINE" class="headerlink" title="DRL项目搭建流程(PIPELINE)"></a>DRL项目搭建流程(PIPELINE)</h3><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195709.png" alt="image-20191125154213166"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195710.png" alt="image-20191125154322290"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195711.png" alt="image-20191125154350506"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195712.png" alt="image-20191125154421405"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195713.png" alt="image-20191125154504303"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195714.png" alt="image-20191125154530205"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195715.png" alt="image-20191125154611740"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195716.png" alt="image-20191125154642408"></p><h3 id="DRL-examples"><a href="#DRL-examples" class="headerlink" title="DRL examples"></a>DRL examples</h3><p><a href="https://gym.openai.com/docs/">Gym-Cart-Pole</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930092157.JPG"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930092247.JPG" alt="42"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930092320.JPG" alt="43"></p><p><strong>更多应用场景：</strong></p><p>自动驾驶: 自动驾驶载具（self-driving vehicle）<br>控制论(离散和连续大动作空间): 玩具直升机、Gymm_cotrol物理部件控制、机器人行走、机械臂控制。<br>游戏: Go, Atari 2600(DeepMind论文详解)等<br>理解机器学习: 自然语言识别和处理, 文本序列预测<br>超参数学习: 神经网络参数自动设计<br>问答系统: 对话系统<br>推荐系统: 阿里巴巴黄皮书（商品推荐），广告投放。<br>智能电网: 电网负荷调试, 调度等<br>通信网络: 动态路由, 流量分配等<br>物理化学实验: 定量实验,核素碰撞,粒子束流调试等<br>程序学习和网络安全: 网络攻防等</p><p><strong>第一课作业：</strong></p><p>下图中根据每个人自己的理解对human life进行抽象，得到Goal、State、Actions、Reward指标。</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930092721.JPG" alt="44"></p><h3 id="keypoints"><a href="#keypoints" class="headerlink" title="keypoints"></a>keypoints</h3><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930092852.JPG" alt="39"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195717.png" alt="image-20191125154932764"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930092935.JPG" alt="40"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195718.png" alt="image-20191125154831799"></p><h3 id="简化模型及MDP"><a href="#简化模型及MDP" class="headerlink" title="简化模型及MDP"></a>简化模型及MDP</h3><h4 id="RL模型简化"><a href="#RL模型简化" class="headerlink" title="RL模型简化"></a>RL模型简化</h4><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195719.png" alt="image-20191125154722065"></p>   <div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930093009.png width="450" height='200' alt='RL简化模型'/></div><br><p>来自Sutton和Barto的书“Reinforcement Learning: an Introduction”（这是强烈推荐的）的这张图，很好的解释了智能体和环境之间的相互作用。在某个时间步t，智能体处于状态s_t，采取动作a_t。然后环境会返回一个新的状态s_t+1和一个奖励r_t+1。奖励处于t+1时间步是因为它是由环境在t+1的状态s_t+1返回的，因此让它们两个保持一致更加合理（如上图所示）。</p><h4 id="MDP模型"><a href="#MDP模型" class="headerlink" title="MDP模型"></a>MDP模型</h4><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930093228.png width="600" height='200' alt='MDP简化模型'/></div><br><p>你不需要一个MDP来告诉自己饿了要吃饭，但是强化学习的机制是需要它的。这个MDP增加了奖励机制，你每转化到一个状态，就会获得一次奖励。在这个例子中，由于接下来状态是饥饿，你会得到一个负面的奖励，如果接下来状态是饿死，那会得到一个更负面的奖励。如果你吃饱了，就会获得一个正面的奖励。现在我们的MDP已经完全成型，我们可以开始思考如何采取行动去获取能获得的最高奖励。由于这个MDP是十分简单的，我们很容易发现待在一个更高奖励的区域的方式，即当我们饥饿的时候就吃。在这个模型中，当我们处于吃饱状态的时候没有太多其它的选择，但是我们将会不可避免的再次饥饿，然后立马选择进食。强化学习感兴趣的问题其实具有更大更复杂的马尔科夫决策过程，并且在我们开始实际探索前，我们通常不知道这些策略。</p><p>百度百科对马尔可夫链和决策过程的解释：</p><p><a href="%5Bhttps://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/6171383%5D(https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E9%93%BE/6171383)">马尔可夫链</a></p><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930093539.JPG width="600" height='400' alt='MDP简化模型'/></div><br><p><a href="%5Bhttps://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/5824810?fr=aladdin%5D(https://baike.baidu.com/item/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/5824810?fr=aladdin)">马尔可夫决策过程</a></p><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930093640.JPG width="600" height='400' alt='MDP简化模型'/></div><br><p><strong>举个例子，一个抛物线小球，给出当前时刻的速度和位置(当前时刻的状态)，下一时刻的小球位置只取觉与当前时刻的小球状态；但是如果只给出当前时刻小球的位置，则下一时刻小球的位置取决于之前所有时刻小球的状态，这时候不具有马尔可夫性质。</strong></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195720.png" alt="image-20191125155054862"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195721.png" alt="image-20191125155145767"></p><p>我们现在已经有一个强化学习问题的框架，接下来准备学习如何最大化奖励函数。在下一部分中，我们将进一步学习状态价值（state value）函数和状态-动作价值（action value）函数，以及奠定了强化学习算法基础的贝尔曼（Bellman）方程，并进一步探索一些简单而有效的动态规划解决方案。</p><h3 id="Bellman-Equation优化"><a href="#Bellman-Equation优化" class="headerlink" title="Bellman Equation优化"></a>Bellman Equation优化</h3><p>推导方式一：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930093727.jpg" alt="20"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930093747.jpg" alt="21"></p><p>推导方式二：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930093834.JPG" alt="52"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094024.png" alt="s5"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094043.JPG" alt="51"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094103.JPG" alt="53"></p><p><strong>同一个状态state下同一策略输出的动作不一定是确定的，(例如石头剪刀布中最优策略是随机策略)，因此在Bellman Equation推导过程中一定注意每一个变量自身又是一个随机变量；同时即使在同一个状态state下根据一个最优的确定性策略输出确定性动作，得到的下一个state也不一定是唯一确定的，这里会有一个状态转移概率(因此在这里会有确定性环境和不确定环境之分，不确定环境就是同一s，a下一个s不确定)。</strong></p><p><strong><u>臆想场景：</u></strong></p><p>策略是一个战略方针，不是具体的战术动作，例如”敌进我退敌驻我扰敌疲我打敌退我追”16字方针就是一个策略，具体的战术动作需要根据当前所处的状态决定；</p><p>1、行军打战到某处，例如灌木从，此时可以有前哨兵勘测地形和敌军状态，例如离我军多远，装备如何，人数，是否是王牌部队等等，继而根据16字方针决定接下来我军的作战策略，是打，是跑还是什么，这个16字方针就可以看作是与敌军不断流血斗争中学到的最优策略；</p><p>2、同一个时间，同一个地点，男生向女生表白，得到的下一个状态可能是被拒绝也有可能是接受；即同一个s，a，下一个状态不一定是确定的；</p><p>3、石头剪刀布中面对对方出布，我方最优策略是随机策略，即三个动作概率是一样的，不然随着游戏的进行，有所偏重的动作就会被针对，一定是很多局；</p><p>4、on-off policy例子，为什么比你高，壮的人，本能是不与之打架，这就是off policy，自己的不打的动作来源于其他人的policy，即其他人打架的与环境的交互行为(结果被暴揍)和自身更新policy不是一个policy；on-policy就是自己不信邪，非要以身尝试，不停的打架最后发现打不过不打了。</p><h3 id="算法归纳"><a href="#算法归纳" class="headerlink" title="算法归纳"></a>算法归纳</h3> <div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094136.png width="600" height='400' alt='RL简化模型'/></div><br><h2 id="Model-based-models"><a href="#Model-based-models" class="headerlink" title="Model-based models"></a>Model-based models</h2><p>基于model的方法是指环境信息可以获取，例如环境状态转移概率等信息，与之对应的是model-free的方法，该类方法无法获取环境的准确信息，例如无法直接获取环境的状态转移概率；</p><p> 对于贝尔曼方程的求解，可以采取动态规划（Dynamic Programming）的方法。具体来说，动态规划有两个方法： 这里主要介绍两个model-based的方法，包括值迭代和策略迭代方法。</p><h3 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h3><p> 算法主要流程如下：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094237.JPG" alt="22"></p><p><a href="https://blog.csdn.net/hhy_csdn/article/details/89091008">示例代码</a></p><p>环境：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094304.JPG" alt="25"></p><p>值迭代函数：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094336.JPG" alt="23"></p><p>策略policy获取：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094403.JPG" alt="24"></p><h3 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h3><p>算法主要流程如下：</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094434.JPG" alt="26"></p><p><a href="https://blog.csdn.net/hhy_csdn/article/details/89096938">示例代码</a></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094457.JPG" alt="27"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094512.JPG" alt="28"></p><p><strong>不管是值迭代还是策略迭代，都是需要计算出最优的值函数，然后通过值函数计算出Q函数，然后根据Q函数计算出每个状态state下的最优动作action。</strong></p><h2 id="Model-free-models"><a href="#Model-free-models" class="headerlink" title="Model-free models"></a>Model-free models</h2><p>很多情况下，环境的模型是未知的，我们不清楚状态之间如何转移，回报的概率是多少，甚至不清楚全部的状态空间长什么样子。这种情况下，如果不采用model-based方法（即，对复杂环境进行建模，建模过程其实就是学习过程，模型建的充分了，智能体对环境就理解充分了，就可以得出最优policy），而是采用model-free的方法（不去对环境进行建模，只用采样的方式学习出一个最优policy），最经典的就是<a href="https://blog.csdn.net/hhy_csdn/article/details/89156891">蒙特卡罗算法</a>了。</p><h3 id="Monte-Carlo-MC-Methods"><a href="#Monte-Carlo-MC-Methods" class="headerlink" title="Monte Carlo(MC) Methods"></a>Monte Carlo(MC) Methods</h3><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094541.JPG" alt="29"></p><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094633.JPG width="300" height='300' alt='RL简化模型'/></div><br><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094751.JPG" alt="31"></p><h3 id="Temporal-Difference-TD-lambda-Methods"><a href="#Temporal-Difference-TD-lambda-Methods" class="headerlink" title="Temporal Difference(TD(lambda)) Methods"></a>Temporal Difference(TD(lambda)) Methods</h3><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094847.JPG" alt="32"></p><p>时间差分(TD)方法主要包括Q-learning和SARSA方法，其中根据Q值更新受影响的步长lambda，有Q-learning(lambda)和SARSA(lambda)。</p><p><strong>注意：这里的Q-learning是off-policy算法，SARSA是on-policy算法。</strong></p><h4 id="Q-learning-methods"><a href="#Q-learning-methods" class="headerlink" title="Q-learning methods"></a>Q-learning methods</h4><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094915.JPG" alt="33"></p><p><a href="https://blog.csdn.net/hhy_csdn/article/details/89157318">代码</a></p><h4 id="SARSA-methods"><a href="#SARSA-methods" class="headerlink" title="SARSA methods"></a>SARSA methods</h4><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930094945.JPG" alt="34"></p><p><a href="https://blog.csdn.net/hhy_csdn/article/details/89216872">代码</a></p><h5 id="超参数分析"><a href="#超参数分析" class="headerlink" title="超参数分析"></a>超参数分析</h5><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095016.JPG" alt="35"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095031.JPG" alt="36"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195722.png" alt="image-20191125190408199"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195723.png" alt="image-20191125190544026"></p><h4 id="Q-learning-SARSA-lambda-methods"><a href="#Q-learning-SARSA-lambda-methods" class="headerlink" title="Q-learning/SARSA(lambda) methods"></a>Q-learning/SARSA(lambda) methods</h4><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095059.JPG width="1000" height='500' alt='RL简化模型'/></div><br><p>Sarsa 是说到做到型, 所以我们也叫他 on-policy, 在线学习, 学着自己在做的事情.Sarsa相当保守,他会选择离危险远远的,拿到宝藏是次要的, 保住自己的小命才是王道. 这就是使用 Sarsa 方法的不同之处.<br>Q learning 是说到但并不一定做到,所以它也叫作 Off-policy,离线学习.而因为有了 maxQ,Q-learning 也是一个特别勇敢的算法.永远都会选择最近的一条通往成功的道路, 不管这条路会有多危险.</p><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095152.JPG width="1000" height='400' alt='RL简化模型'/></div><br><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095220.JPG width="1000" height='400' alt='RL简化模型'/></div><br><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095249.JPG width="1000" height='300' alt='RL简化模型'/></div><br><div align=center><img data-src=https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095319.JPG width="1000" height='300' alt='RL简化模型'/></div><br><p><strong>注意lambda是0时，是单步更新，lambda是1时，是回合更新，lambda在0-1之间时，是成一个衰减趋势，离更新近的权重大，远的步权重小。</strong></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095350.png" alt="s3"></p><p>DP、MC、TD三种方法对比：<br>1、DP基于模型; MC、TD基于无模型<br>2、DP采用bootstrapping(自举), MC采用采样，TD采用bootstrapping+采样<br>3、DP用后继状态的值函数估计当前值函数，MC利用经验平均估计状态的值函数，TD利用后继状态的值函数估计当前值函数<br>4、MC和TD都是利用样本估计值函数，其中MC为无偏估计，TD为有偏估计<br>5、最明显的就是下图的值函数的计算方法的不同</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095416.JPG" alt="40"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200928195724.png"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095445.png" alt="s1"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095459.png" alt="s4"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095515.JPG" alt="57"></p><p><a href="https://www.jianshu.com/p/40aff6fad7b9">特点：</a></p><p>  （1）不断试错<br>  （2）看重长期回报 </p><h2 id="QA"><a href="#QA" class="headerlink" title="QA"></a>QA</h2><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>1、<a href="https://blog.csdn.net/qq_30615903/article/details/80746553"> https://blog.csdn.net/qq_30615903/article/details/80746553 </a></p><p>2、<a href="https://www.lizenghai.com/archives/20955.html"> https://www.lizenghai.com/archives/20955.html </a></p><p>3、Reinforcement learning：An Introduction</p><p>4、<a href="https://www.bilibili.com/video/av16921335/#page=22">视频课程</a></p><p>5、…………待添加</p><h1 id="第二章"><a href="#第二章" class="headerlink" title="第二章"></a>第二章</h1><p>本章主要从Deep Reinforcement learning(DRL)基本概念出发，介绍深度强化学习的一些经典算法并辅以公式推导和程序实现。</p><h2 id="DRL基本概念"><a href="#DRL基本概念" class="headerlink" title="DRL基本概念"></a>DRL基本概念</h2><p>传统的强化学习局限于动作空间和样本空间都很小，且一般是离散的情境下。然而比较复杂的、更加接近实际情况的任务则往往有着很大的状态空间和连续的动作空间。当输入数据为图像，声音时，往往具有很高维度，传统的强化学习很难处理，深度强化学习就是把深度学习对于的高维输入与强化学习结合起来。 </p><h2 id="DRL-经典算法"><a href="#DRL-经典算法" class="headerlink" title="DRL 经典算法"></a>DRL 经典算法</h2><p> 2013和2015年DeepMind的Deep Q Network（DQN）可谓是将两者成功结合的开端，它用一个深度网络代表价值函数，依据强化学习中的Q-Learning，为深度网络提供目标值，对网络不断更新直至收敛， 后续还有很多基于DQN的改进版本。<strong>但是DQN主要用于解决离散动作空间的问题，无法解决高维连续动作空间，而PG不仅可以用于解决高维连续动作空间，也可以解决离散动作空间问题。同时将PG和值函数两者的优势相结合得到的Actor-Critic框架系列方法性能得到进一步提升。</strong>  </p><p><strong>DDPG可以学习随机策略和连续动作，原始的DQN则无法学习随机策略和连续动作。</strong></p><p><strong>随机策略(非确定性策略)vs确定性策略</strong></p><p> 那么什么是策略呢？ 通常情况下被定义为是从状态到行为的一个映射，<strong>直白说就是每个状态下指定一个动作概率，这个可以是确定性的（一个确定动作），也可以是不确定性的。</strong> </p><p>策略是一个战略方针，不是具体的战术动作，例如”敌进我退敌驻我扰敌疲我打敌退我追”就是一个策略，具体的战术动作需要根据当前所处的状态决定；</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095551.JPG" alt="50"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095608.JPG" alt="54"></p><p><strong>离散动作vs连续动作</strong></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095630.JPG" alt="55"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930095643.JPG" alt="56"></p><h3 id="DRL-Model-free-Methods"><a href="#DRL-Model-free-Methods" class="headerlink" title="DRL Model-free Methods"></a>DRL Model-free Methods</h3><p>下面介绍的DQN系列算法、DDPG系列算法和AC系列算法属于model-free的范畴，即不需要对环境建模，主要通过与环境交互获取抽样样本进行模型的训练。</p><h4 id="Deep-Q-learning-Network-DQN"><a href="#Deep-Q-learning-Network-DQN" class="headerlink" title="Deep Q-learning Network(DQN)"></a>Deep Q-learning Network(DQN)</h4><p>下图直观展示DQN算法训练后的吃豆豆的游戏agent。</p><div align=center><img data-src=https://raw.githubusercontent.com/yangsuhui/PicGoPictureBed/master/img/20200930103954.gif width="400" height='400' alt='RL简化模型'/></div><br><p>作为深度强化学习领域的重要开创性工作，DQN的出现引发了众多研究团队的关注。在文献[1]中，介绍了DQN早期的主要改进工作，包括大规模分布式DQN[14]、双重DQN[15]、带优先级经验回放的DQN[16]、竞争架构DQN[17]、引导DQN[18]以及异步DQN[19]等。这些工作从不同角度改进DQN的性能。</p><p>此后，研究人员又陆续提出了一些DQN的重要扩展，继续完善DQN算法。Zhao等基于在策略(on-policy)强化学习，提出了深度SARSA(state-action-reward-state-action)算法[20]。实验证明在一些Atari视频游戏上，深度SARSA算法的性能要优于DQN。Anschel等提出了平均DQN，通过取Q值的期望以降低目标值函数的方差，改善了深度强化学习算法的不稳定性[21]。实验结果表明，平均DQN在ALE测试平台上的效果要优于DQN和双重DQN。He等在DQN的基础上提出一种约束优化算法来保证策略最优和奖赏信号快速传播[22]。该算法极大提高了DQN的训练速度，在ALE平台上经过一天训练就达到了DQN和双重DQN经过十天训练的效果。作为DQN的一种变体，分类DQN算法从分布式的角度分析深度强化学习[23]。与传统深度强化学习算法中选取累积奖赏的期望不同，分类DQN将奖赏看作一个近似分布，并且使用贝尔曼等式学习这个近似分布。分类DQN算法在Atari视频游戏上的平均表现要优于大部分基准算法。深度强化学习中参数的噪声可以帮助算法更有效地探索周围的环境，加入参数噪声的训练算法可以大幅提升模型的效果，并且能更快地教会智能体执行任务。噪声DQN在动作空间中借助噪声注入进行探索性行为，结果表明带有参数噪声的深度强化学习将比分别带有动作空间参数和进化策略的传统强化学习效率更高[24]。彩虹(Rainbow)将各类DQN的算法优势集成在一体，取得目前最优的算法性能，视为DQN算法的集大成者[25]。DQN算法及其主要扩展如下图所示：</p><p><img data-src="https://raw.githubusercontent.com/yangsuhui/PicGoPictureBed/master/img/20200930125500.jpg" alt="s10"></p><h5 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h5><h5 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h5><h5 id="Prioritised-replay"><a href="#Prioritised-replay" class="headerlink" title="Prioritised replay"></a>Prioritised replay</h5><h5 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h5><h5 id="RainBow"><a href="#RainBow" class="headerlink" title="RainBow"></a>RainBow</h5><h4 id="Deep-Deterministic-Policy-Gradient-DDPG"><a href="#Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="Deep Deterministic Policy Gradient(DDPG)"></a>Deep Deterministic Policy Gradient(DDPG)</h4><p><strong>基于值函数的深度强化学习主要应用于离散动作空间的任务。面对连续动作空间的任务，基于策略梯度的深度强化学习算法能获得更好的决策效果。</strong> </p><p>==分类：在策略/离策略梯度；随机/确定性策略梯度；==</p><p>目前<strong>的大部分actor-critic算法都是采用在策略的强化学习算法。这意味着无论使用何种策略进行学习，critic部分都需要根据当前actor的输出作用于环境产生的反馈信号才能学习</strong>。因此，<strong>在策略类型</strong>的actor-critic算法是<strong>无法使用类似于经验回放的技术提升学习效率的，也由此带来训练的不稳定和难以收敛性</strong>。Lillicrap等提出的<strong>深度确定性策略梯度算法(deep deterministic policy gradient，DDPG)，将DQN算法在离散控制任务上的成功经验应用到连续控制任务的研究[30]。DDPG是无模型、离策略(offpolicy)的actor-critic算法</strong>，使用深度神经网络作为逼近器，将**深度学习和确定性策略梯度算法有效地结合在一起。DDPG源于确定性策略梯度(determinist policy gradient，DPG)算法[31]**。确定性策略记为πθ(s)，表示状态S和动作A在参数θ的策略作用下得到S7→A。期望奖赏J(π)如下所示:</p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125658.JPG" alt="58"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125725.JPG" alt="59"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125741.JPG" alt="60"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125758.JPG" alt="61"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125817.JPG" alt="62"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125832.png" alt="s7"></p><h5 id="Policy-gradient"><a href="#Policy-gradient" class="headerlink" title="Policy gradient"></a>Policy gradient</h5><h5 id="Stochastic-policy-gradient-SPG-vs-determinist-policy-gradient-DPG"><a href="#Stochastic-policy-gradient-SPG-vs-determinist-policy-gradient-DPG" class="headerlink" title="Stochastic policy gradient (SPG)  vs  determinist policy gradient (DPG)"></a>Stochastic policy gradient (SPG)  vs  determinist policy gradient (DPG)</h5><h5 id="DDPG"><a href="#DDPG" class="headerlink" title="DDPG"></a>DDPG</h5><h5 id="Trust-region-policy-optimization-TRPO"><a href="#Trust-region-policy-optimization-TRPO" class="headerlink" title="Trust region policy optimization (TRPO)"></a>Trust region policy optimization (TRPO)</h5><h5 id="PPO"><a href="#PPO" class="headerlink" title="PPO"></a>PPO</h5><h4 id="Actor-Critic-Series-Method-AC"><a href="#Actor-Critic-Series-Method-AC" class="headerlink" title="Actor Critic Series Method(AC)"></a>Actor Critic Series Method(AC)</h4><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125918.png" alt="s6"></p><h5 id="AC"><a href="#AC" class="headerlink" title="AC"></a>AC</h5><h5 id="A2C"><a href="#A2C" class="headerlink" title="A2C"></a>A2C</h5><h5 id="A3C"><a href="#A3C" class="headerlink" title="A3C"></a>A3C</h5><h5 id="UNREAL"><a href="#UNREAL" class="headerlink" title="UNREAL"></a>UNREAL</h5><h3 id="DRL-Model-based-Methods"><a href="#DRL-Model-based-Methods" class="headerlink" title="DRL Model-based Methods"></a>DRL Model-based Methods</h3><h4 id="World-Model"><a href="#World-Model" class="headerlink" title="World Model"></a>World Model</h4><h4 id="PlaNet"><a href="#PlaNet" class="headerlink" title="PlaNet"></a>PlaNet</h4><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930125951.png" alt="s8"></p><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930130009.png" alt="s1"></p><h2 id="QA-1"><a href="#QA-1" class="headerlink" title="QA"></a>QA</h2><h2 id="参考资料-1"><a href="#参考资料-1" class="headerlink" title="参考资料"></a>参考资料</h2><h1 id="第三章"><a href="#第三章" class="headerlink" title="第三章"></a>第三章</h1><h2 id="Environmental-interaction-framework"><a href="#Environmental-interaction-framework" class="headerlink" title="Environmental interaction framework"></a>Environmental interaction framework</h2><p>gym只支持linux，Tkinter可以支持win/linux</p><h3 id="OpenAI-gym"><a href="#OpenAI-gym" class="headerlink" title="OpenAI gym"></a><a href="https://github.com/openai/gym">OpenAI gym</a></h3><h3 id="Tkinter-可学-你可以自己用它来编写模拟环境"><a href="#Tkinter-可学-你可以自己用它来编写模拟环境" class="headerlink" title="Tkinter (可学), 你可以自己用它来编写模拟环境"></a><a href="https://morvanzhou.github.io/tutorials/python-basic/tkinter/">Tkinter</a> (可学), 你可以自己用它来编写模拟环境</h3><h2 id="AlphaGo-Series"><a href="#AlphaGo-Series" class="headerlink" title="AlphaGo Series"></a>AlphaGo Series</h2><p><img data-src="https://gitee.com/yangsuhui_i/pic-go-picture-bed/raw/master/imgs/deep_RL/20200930130047.png" alt="s9"></p><h3 id="AlphaGo"><a href="#AlphaGo" class="headerlink" title="AlphaGo"></a>AlphaGo</h3><h3 id="AlphaGo-Master"><a href="#AlphaGo-Master" class="headerlink" title="AlphaGo Master"></a>AlphaGo Master</h3><h3 id="AlphaGo-Zero"><a href="#AlphaGo-Zero" class="headerlink" title="AlphaGo Zero"></a>AlphaGo Zero</h3><h3 id="AlphaZero"><a href="#AlphaZero" class="headerlink" title="AlphaZero"></a>AlphaZero</h3><h3 id="MuZero"><a href="#MuZero" class="headerlink" title="MuZero"></a>MuZero</h3><h2 id="DRL的主要研究方向"><a href="#DRL的主要研究方向" class="headerlink" title="DRL的主要研究方向"></a>DRL的主要研究方向</h2><h3 id="科学研究方向"><a href="#科学研究方向" class="headerlink" title="科学研究方向"></a>科学研究方向</h3><h4 id="Reward-Shaping"><a href="#Reward-Shaping" class="headerlink" title="Reward Shaping"></a>Reward Shaping</h4><h4 id="Multi-Agent-DRL"><a href="#Multi-Agent-DRL" class="headerlink" title="Multi-Agent DRL"></a>Multi-Agent DRL</h4><h5 id="MADDPG"><a href="#MADDPG" class="headerlink" title="MADDPG"></a>MADDPG</h5><h4 id="带有迁移属性的强化学习"><a href="#带有迁移属性的强化学习" class="headerlink" title="带有迁移属性的强化学习"></a>带有迁移属性的强化学习</h4><h4 id="元强化学习"><a href="#元强化学习" class="headerlink" title="元强化学习"></a>元强化学习</h4><h4 id="分层强化学习"><a href="#分层强化学习" class="headerlink" title="分层强化学习"></a>分层强化学习</h4><h4 id="强化学习与神经生物学的联系"><a href="#强化学习与神经生物学的联系" class="headerlink" title="强化学习与神经生物学的联系"></a>强化学习与神经生物学的联系</h4><h3 id="应用研究方向"><a href="#应用研究方向" class="headerlink" title="应用研究方向"></a>应用研究方向</h3><h4 id="Recommendation-system-with-DRL"><a href="#Recommendation-system-with-DRL" class="headerlink" title="Recommendation system with DRL"></a>Recommendation system with DRL</h4><h4 id="智能调度-派单-系统"><a href="#智能调度-派单-系统" class="headerlink" title="智能调度(派单)系统"></a>智能调度(派单)系统</h4><h2 id="Competitions"><a href="#Competitions" class="headerlink" title="Competitions"></a>Competitions</h2><h2 id="Other-related-works"><a href="#Other-related-works" class="headerlink" title="Other related works"></a>Other related works</h2><h3 id="进化算法"><a href="#进化算法" class="headerlink" title="进化算法"></a>进化算法</h3><h3 id="蚁群算法"><a href="#蚁群算法" class="headerlink" title="蚁群算法"></a>蚁群算法</h3><h3 id="模拟退火算法"><a href="#模拟退火算法" class="headerlink" title="模拟退火算法"></a>模拟退火算法</h3><h2 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h2><h2 id="QA-2"><a href="#QA-2" class="headerlink" title="QA"></a>QA</h2><h2 id="参考资料-2"><a href="#参考资料-2" class="headerlink" title="参考资料"></a>参考资料</h2>]]></content>
    
    
    <summary type="html">&lt;p&gt;本文主要整理深度强化学习的相关内容。&lt;/p&gt;</summary>
    
    
    
    <category term="Deep Learning" scheme="https://yangsuhui.github.io/categories/Deep-Learning/"/>
    
    
    <category term="RL" scheme="https://yangsuhui.github.io/tags/RL/"/>
    
  </entry>
  
</feed>
